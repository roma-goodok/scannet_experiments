{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roma/.virtualenvs/aseg_torch1/lib/python3.6/site-packages/kekas/keker.py:9: UserWarning: Error 'No module named 'apex''' during importing apex library. To use mixed precison you should install it from https://github.com/NVIDIA/apex\n",
      "  warnings.warn(f\"Error '{e}'' during importing apex library. To use mixed precison\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "#import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from pdb import set_trace as st\n",
    "\n",
    "#import pretrainedmodels as pm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "#from albumentations import Compose, JpegCompression, CLAHE, RandomRotate90, Transpose, ShiftScaleRotate, \\\n",
    "#        Blur, OpticalDistortion, GridDistortion, HueSaturationValue, Flip, VerticalFlip\n",
    "\n",
    "from kekas import Keker, DataOwner, DataKek\n",
    "from kekas.transformations import Transformer, to_torch, normalize\n",
    "from kekas.metrics import accuracy\n",
    "from kekas.modules import Flatten, AdaptiveConcatPool2d\n",
    "from kekas.callbacks import Callback, Callbacks, DebuggerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai_sparse # 3D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparseconvnet as scn\n",
    "\n",
    "from fastai_sparse import utils, visualize\n",
    "from fastai_sparse.utils import log\n",
    "#from fastai_sparse.data import DataSourceConfig, MeshesDataset, SparseDataBunch\n",
    "#from fastai_sparse.learner import SparseModelConfig, Learner\n",
    "#from fastai_sparse.callbacks import TimeLogger, SaveModelCallback, CSVLogger\n",
    "from fastai_sparse.transforms import Transform, Compose\n",
    "\n",
    "from data import merge_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment environment and system metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neptune_callbacks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-31ec532af154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneptune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneptune_callbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeptuneMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neptune_callbacks'"
     ]
    }
   ],
   "source": [
    "import neptune\n",
    "from neptune_callbacks import NeptuneMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_epoch': 384, 'max_lr': 0.5, 'wd': 3e-06}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params={'n_epoch': 256+128,\n",
    "        'max_lr': 0.5,\n",
    "        'wd':0.000003\n",
    "        }\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "with open('NEPTUNE_API_TOKEN.txt','r') as f:\n",
    "    NEPTUNE_API_TOKEN = f.readline().splitlines()[0]\n",
    "    \n",
    "neptune.init(api_token=NEPTUNE_API_TOKEN,\n",
    "             project_qualified_name='roma-goodok/fastai-sparse-scannet')\n",
    "\n",
    "\n",
    "\n",
    "# create experiment in the project defined above\n",
    "exp = neptune.create_experiment(params=params)\n",
    "print(exp.id)\n",
    "exp.append_tag('study')\n",
    "exp.append_tag('kekas')\n",
    "exp.append_tag('unet24')\n",
    "exp.append_tag('1cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kekas'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    experiment_name = exp.id\n",
    "except Exception as e:\n",
    "    experiment_name = \"kekas\"\n",
    "experiment_name        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virtualenv:     (aseg_torch1) \n",
      "python:         3.6.8\n",
      "nvidia driver:  b'384.130'\n",
      "nvidia cuda:    9.0, V9.0.176\n",
      "cudnn:          7.1.4\n",
      "torch:          1.0.0\n",
      "pandas:         0.24.2\n",
      "kekas:          0.1.17\n",
      "fastai:         1.0.48\n",
      "fastai_sparse:  0.0.4.dev0\n"
     ]
    }
   ],
   "source": [
    "utils.watermark(pandas=True, kekas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33ma24855f\u001b[m Updates\r\n",
      "\u001b[33m87b37dc\u001b[m Initial commit\r\n",
      "\u001b[33m87cb41a\u001b[m Initial commit\r\n"
     ]
    }
   ],
   "source": [
    "!git log1 -n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 22:48:30 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 384.130                Driver Version: 384.130                   |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:28:00.0  On |                  N/A |\r\n",
      "| 35%   56C    P0    69W / 250W |   1431MiB / 11163MiB |      4%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:29:00.0 Off |                  N/A |\r\n",
      "|  0%   52C    P8    18W / 250W |     11MiB / 11172MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1442      G   /usr/lib/xorg/Xorg                           584MiB |\r\n",
      "|    0      2538      G   compiz                                       358MiB |\r\n",
      "|    0      5234      G   ...quest-channel-token=6448628850995738348    87MiB |\r\n",
      "|    0     20121      G   /usr/lib/firefox/firefox                     144MiB |\r\n",
      "|    0     20209      G   /usr/lib/firefox/firefox                     157MiB |\r\n",
      "|    0     25241      G   ...-token=08F1834031BE00480CD0EB1E3A687B0E    95MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:            AMD Ryzen 7 1700 Eight-Core Processor\r\n"
     ]
    }
   ],
   "source": [
    "!lscpu | grep \"Model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.wide_notebook()\n",
    "# uncomment this lines if you want switch off interactive and save visaulisation as screenshoots:\n",
    "# For rendering run command in terminal:    `chromium-browser --remote-debugging-port=9222`\n",
    "if  False:\n",
    "    visualize.options.interactive = False\n",
    "    visualize.options.save_images = True\n",
    "    visualize.options.verbose = True\n",
    "    visualize.options.filename_pattern_image = Path('images', experiment_name, 'fig_{fig_number}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see how download and preprocess data by the following link from fastai_sparse library: https://github.com/goodok/fastai_sparse/tree/master/examples/scannet/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scene0000_01.merged.ply']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOURCE_DIR = Path('data', 'scannet_merged_ply')\n",
    "assert SOURCE_DIR.exists(), \"Run prepare_data.ipynb\"\n",
    "\n",
    "definition_of_spliting_dir = Path('data', 'ScanNet_Tasks_Benchmark')\n",
    "assert definition_of_spliting_dir.exists()\n",
    "\n",
    "\n",
    "os.listdir(SOURCE_DIR / 'scene0000_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(path, ext='merged.ply'):\n",
    "    pattern = str(path / '*' / ('*' + ext))\n",
    "    fnames = glob.glob(pattern)\n",
    "    return fnames\n",
    "\n",
    "def get_df_list(verbose=0):\n",
    "    # train /valid / test splits\n",
    "    fn_lists = {}\n",
    "\n",
    "    fn_lists['train'] = definition_of_spliting_dir / 'scannetv1_train.txt'\n",
    "    fn_lists['valid'] = definition_of_spliting_dir / 'scannetv1_val.txt'\n",
    "    fn_lists['test'] = definition_of_spliting_dir / 'scannetv1_test.txt'\n",
    "\n",
    "    for datatype in ['train', 'valid', 'test']:\n",
    "        assert fn_lists[datatype].exists(), datatype\n",
    "\n",
    "    dfs = {}\n",
    "    total = 0\n",
    "    for datatype in ['train', 'valid', 'test']:\n",
    "        df = pd.read_csv(fn_lists[datatype], header=None, names=['example_id'])\n",
    "        df = df.assign(datatype=datatype)\n",
    "        df = df.assign(subdir=df.example_id)\n",
    "        df = df.sort_values('example_id')\n",
    "        dfs[datatype] = df\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{datatype:5} counts: {len(df):>4}\")\n",
    "        \n",
    "        total += len(df)\n",
    "    if verbose:\n",
    "        print(f\"total:     {total}\")\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train counts: 1045\n",
      "valid counts:  156\n",
      "test  counts:  312\n",
      "total:     1513\n"
     ]
    }
   ],
   "source": [
    "df_list = get_df_list(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>datatype</th>\n",
       "      <th>subdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>scene0000_00</td>\n",
       "      <td>train</td>\n",
       "      <td>scene0000_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>scene0000_01</td>\n",
       "      <td>train</td>\n",
       "      <td>scene0000_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>scene0000_02</td>\n",
       "      <td>train</td>\n",
       "      <td>scene0000_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>scene0001_00</td>\n",
       "      <td>train</td>\n",
       "      <td>scene0001_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>scene0001_01</td>\n",
       "      <td>train</td>\n",
       "      <td>scene0001_01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       example_id datatype        subdir\n",
       "827  scene0000_00    train  scene0000_00\n",
       "828  scene0000_01    train  scene0000_01\n",
       "829  scene0000_02    train  scene0000_02\n",
       "496  scene0001_00    train  scene0001_00\n",
       "497  scene0001_01    train  scene0001_01"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list['train'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scene0000_00.merged.ply']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(SOURCE_DIR, 'scene0000_00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_sparse.data_items  import MeshItem, PointsItem\n",
    "from fastai_sparse.learner import SparseModelConfig\n",
    "import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at first we need to create a reader function that will define how image will be opened\n",
    "def reader_fn(i, row):\n",
    "    fn = SOURCE_DIR / row['subdir'] / f'{row[\"example_id\"]}.merged.ply'\n",
    "    m = MeshItem.from_file(fn, label_field='label')\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeshItem (scene0000_00.merged.ply)\n",
      "vertices:                shape: (81369, 3)            dtype: float64        min:   -0.01657,  max:    8.74040,  mean:    3.19051\n",
      "faces:                   shape: (153587, 3)           dtype: int64          min:          0,  max:      81368,  mean: 40549.68796\n",
      "colors:                  shape: (81369, 4)            dtype: uint8          min:    1.00000,  max:  255.00000,  mean:  145.80430\n",
      "labels:                  shape: (81369,)              dtype: uint16         min:    0.00000,  max:  230.00000,  mean:   12.97057\n",
      "Colors from vertices\n",
      "Labels from vertices\n"
     ]
    }
   ],
   "source": [
    "m = reader_fn(0, df_list['train'].iloc[0])\n",
    "m.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.is_colors_from_vertices, m.is_labels_from_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map relevant classes to {0,1,...,19}, and ignored classes to -100\n",
    "remapper = np.ones(3000, dtype=np.int32) * (-100)\n",
    "for i, x in enumerate([1,2,3,4,5,6,7,8,9,10,11,12,14,16,24,28,33,34,36,39]):\n",
    "    remapper[x] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TFMS = [T.to_points_cloud(method='vertices', normals=False), \n",
    "            T.remap_labels(remapper=remapper, inplace=False),\n",
    "            T.colors_normalize(),\n",
    "            T.normalize_spatial(),\n",
    "           ]\n",
    "\n",
    "_scale = 20\n",
    "\n",
    "AUGS_TRAIN = [\n",
    "    T.noise_affine(amplitude=0.1),\n",
    "    T.flip_x(p=0.5),\n",
    "    T.scale(scale=_scale),\n",
    "    T.rotate_XY(),\n",
    "    \n",
    "    T.elastic(gran=6 * _scale // 50, mag=40 * _scale / 50),\n",
    "    T.elastic(gran=20 * _scale // 50, mag=160 * _scale / 50),\n",
    "    \n",
    "    T.specific_translate(full_scale=4096),\n",
    "    T.crop_points(low=0, high=4096),\n",
    "    T.colors_noise(amplitude=0.1),\n",
    "]\n",
    "\n",
    "AUGS_VALID = [\n",
    "    T.noise_affine(amplitude=0.1),\n",
    "    T.flip_x(p=0.5),\n",
    "    T.scale(scale=_scale),\n",
    "    T.rotate_XY(),\n",
    "\n",
    "    T.translate(offset=4096 / 2),\n",
    "    T.rand_translate(offset=(-2, 2, 3)),  # low, high, dimention\n",
    "    \n",
    "    T.specific_translate(full_scale=4096),\n",
    "    T.crop_points(low=0, high=4096),\n",
    "    T.colors_noise(amplitude=0.1),\n",
    "        \n",
    "    ]\n",
    "\n",
    "SPARSE_TFMS = [\n",
    "    T.merge_features(ones=False, colors=True, normals=False),\n",
    "    T.to_sparse_voxels(),\n",
    "]\n",
    "\n",
    "\n",
    "# reimplement to_torch\n",
    "def _to_torch(x):\n",
    "    x.coords \n",
    "    x.features\n",
    "    x.labels\n",
    "    \n",
    "    return x\n",
    "\n",
    "# to_torch = Transform(_to_torch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function SparseDataBunch.merge_fn at 0x7f3b939e4598>, keys_lists=['id', 'labels_raw', 'filtred_mask', 'random_seed', 'num_points'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import merge_fn\n",
    "merge_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(dataset_key):\n",
    "        \n",
    "    return  Compose(PRE_TFMS + AUGS_TRAIN + SPARSE_TFMS), Compose(PRE_TFMS + AUGS_VALID + SPARSE_TFMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataKeks creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_list['train']\n",
    "val_df = df_list['valid']\n",
    "\n",
    "# now let's create DataKeks\n",
    "train_tfms, val_tfms = get_transforms(\"mesh\")\n",
    "\n",
    "train_dk = DataKek(df=train_df, reader_fn=reader_fn, transforms=train_tfms)\n",
    "val_dk = DataKek(df=val_df, reader_fn=reader_fn, transforms=val_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: scene0000_00.merged\n",
      "coords                   shape: (81369, 3)            dtype: int64          min:        349,  max:       4031,  mean: 1669.08843\n",
      "features                 shape: (81369, 3)            dtype: float32        min:   -1.09001,  max:    1.06099,  mean:   -0.19566\n",
      "x                        shape: (81369,)              dtype: int64          min:        603,  max:        765,  mean:  679.72172\n",
      "y                        shape: (81369,)              dtype: int64          min:       3863,  max:       4031,  mean: 3943.69753\n",
      "z                        shape: (81369,)              dtype: int64          min:        349,  max:        447,  mean:  383.84605\n",
      "labels                   shape: (81369,)              dtype: int64          min:       -100,  max:         17,  mean:  -48.51506\n",
      "voxels: 55154\n",
      "points / voxels: 1.4753055082133661\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de11dc3c9ef74cc597d2f5b034a94807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = train_dk[0]\n",
    "b.describe()\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and DataLoaders\n",
    "batch_size = 4\n",
    "workers = 4\n",
    "\n",
    "train_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=workers, shuffle=True, drop_last=True, collate_fn=merge_fn)\n",
    "val_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=workers, shuffle=False, collate_fn=merge_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_dl):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coords': tensor([[3861,  173, 3632,    0],\n",
       "         [3861,  173, 3632,    0],\n",
       "         [3860,  174, 3632,    0],\n",
       "         ...,\n",
       "         [2904, 1623,  485,    3],\n",
       "         [2905, 1622,  484,    3],\n",
       "         [2904, 1622,  484,    3]]),\n",
       " 'features': tensor([[-0.4175, -0.4925, -0.2817],\n",
       "         [-0.4331, -0.5239, -0.2895],\n",
       "         [-0.4253, -0.5161, -0.2895],\n",
       "         ...,\n",
       "         [ 0.5567,  0.5725,  0.0537],\n",
       "         [ 0.6116,  0.6195,  0.1086],\n",
       "         [ 0.5881,  0.5725,  0.1008]]),\n",
       " 'labels': tensor([-100, -100, -100,  ..., -100, -100, -100]),\n",
       " 'id': ['scene0232_00.merged',\n",
       "  'scene0468_02.merged',\n",
       "  'scene0601_00.merged',\n",
       "  'scene0279_02.merged'],\n",
       " 'labels_raw': [array([-100, -100, -100, -100, ...,    6, -100, -100,    0], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ...,    0,    0,    0,    0], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ...,    0,    0,    0,    0], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ..., -100, -100, -100, -100], dtype=int32)],\n",
       " 'filtred_mask': [array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True])],\n",
       " 'random_seed': ['1286829914_120',\n",
       "  '2092029622_504',\n",
       "  '80130284_604',\n",
       "  '1792758101_76'],\n",
       " 'num_points': [81157, 166396, 209655, 237196]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(694404, 694404)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch['coords']), sum(batch['num_points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseModelConfig;\n",
       "   spatial_size: 4096\n",
       "   dimension: 3\n",
       "   block_reps: 1\n",
       "   m: 16\n",
       "   num_planes: [16, 32, 48, 64, 80, 96, 112]\n",
       "   residual_blocks: False\n",
       "   num_classes: 20\n",
       "   num_input_features: 3\n",
       "   mode: 4\n",
       "   downsample: [2, 2]\n",
       "   bias: False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spatial_size  is full_scale\n",
    "model_config = SparseModelConfig(spatial_size=4096, num_classes=20, num_input_features=3, mode=4,\n",
    "                                 m=16, num_planes_coeffs=[1, 2, 3, 4, 5, 6, 7])\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        nn.Module.__init__(self)\n",
    "        self.sparseModel = scn.Sequential(\n",
    "            scn.InputLayer(cfg.dimension, cfg.spatial_size, mode=cfg.mode),\n",
    "            scn.SubmanifoldConvolution(cfg.dimension, nIn=cfg.num_input_features, nOut=cfg.m, filter_size=3, bias=cfg.bias),\n",
    "            scn.UNet(cfg.dimension, cfg.block_reps, cfg.num_planes, residual_blocks=cfg.residual_blocks, downsample=cfg.downsample),\n",
    "            scn.BatchNormReLU(cfg.m),\n",
    "            scn.OutputLayer(cfg.dimension),\n",
    "        )\n",
    "        self.linear = nn.Linear(cfg.m, cfg.num_classes)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        x = [xb['coords'], xb['features']]\n",
    "        x = self.sparseModel(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "model = Model(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the three whales of your pipelane are: the data, the model and the loss (hi, Jeremy)\n",
    "\n",
    "# the data is represented in Kekas by DataOwner. It is a namedtuple with three fields:\n",
    "# 'train_dl', 'val_dl', 'test_dl'\n",
    "# For training process we will need at least two of them, and we can skip 'test_dl' for now\n",
    "# so we will initialize it with `None` value.\n",
    "dataowner = DataOwner(train_dl, val_dl, None)\n",
    "\n",
    "# model is just a pytorch nn.Module, that we created vefore\n",
    "#model = Net(num_classes=2)\n",
    "\n",
    "# loss or criterion is also a pytorch nn.Module. For multiloss scenarios it can be a list of nn.Modules\n",
    "# for our simple example let's use the standart cross entopy criterion\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also we need to specify, what model will do with each batch of data on each iteration\n",
    "# We should define a `step_fn` function\n",
    "# The code below repeats a `keker.default_step_fn` code to provide you with a concept of step function\n",
    "\n",
    "def step_fn(model: torch.nn.Module,\n",
    "            batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Determine what your model will do with your data.\n",
    "\n",
    "    Args:\n",
    "        model: the pytorch module to pass input in\n",
    "        batch: the batch of data from the DataLoader\n",
    "\n",
    "    Returns:\n",
    "        The models forward pass results\n",
    "    \"\"\"\n",
    "    \n",
    "    # you could define here whatever logic you want\n",
    "    inp = batch  # here we get an \"image\" from our dataset\n",
    "    return model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous preparations was mostly out of scope of Kekas library (except DataKeks creation)\n",
    "# Now let's dive into kekas a little bit\n",
    "\n",
    "# firstly, we create a Keker - the core Kekas class, that provides all the keks for your pipeline\n",
    "keker = Keker(model=model,\n",
    "              dataowner=dataowner,\n",
    "              criterion=criterion,\n",
    "              step_fn=step_fn,                    # previosly defined step function\n",
    "              target_key=\"labels\",                 # remember, we defined it in the reader_fn for DataKek?              \n",
    "              opt=torch.optim.Adam,               # optimizer class. if note specifiyng, \n",
    "                                                  # an SGD is using by default\n",
    "              opt_params={\"weight_decay\": 1e-5})  # optimizer kwargs in dict format (optional too)\n",
    "\n",
    "# Actually, there are a lot of params for kekers, but this out of scope of this example\n",
    "# you can read about them in Keker's docstring (but who really reads the docs, huh?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before the start of the finetuning procedure let's freeeze all the layers except the last one - the head\n",
    "# the `freeze` method is mostly inspired (or stolen) from fastai\n",
    "# but you should define a model's attribute to deal with\n",
    "# for example, our model is actually model.net, so we need to specify the 'net' attr\n",
    "# also this method does not freezes batchnorm layers by default. To change this set `freeze_bn=True`\n",
    "\n",
    "#keker.freeze(model_attr=\"net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100% 261/261 [03:22<00:00,  1.83it/s, loss=1.0766]\n",
      "End of LRFinder\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's find an 'optimal' learning rate with learning rate find procedure\n",
    "# for details please see the fastai course and this articles:\n",
    "# https://arxiv.org/abs/1803.09820\n",
    "# https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n",
    "\n",
    "# NOTE: this is an optional step and you can skip it and use your favorite learning rate\n",
    "\n",
    "# you MUST specify the logdir to see graphics\n",
    "# keker will write a tensorboard logs into this folder\n",
    "# to see them start a tensorboard with `--logdir /path/to/logdir`\n",
    "# OR you can use keker.plot_kek_lr method (see cell below)\n",
    "keker.kek_lr(final_lr=0.1, logdir=\"logdir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "name": "train/batch/loss",
         "type": "scatter",
         "uid": "0dc00cca-6111-4a2a-9f4f-20b86cd83c04",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260
         ],
         "y": [
          3.0047085285186768,
          2.9911835193634033,
          3.0384418964385986,
          3.077932834625244,
          2.96917724609375,
          3.024160146713257,
          2.9735915660858154,
          3.0246260166168213,
          2.9587104320526123,
          3.048039436340332,
          3.040461778640747,
          3.056180238723755,
          3.0222675800323486,
          3.060436487197876,
          3.1094753742218018,
          3.054736614227295,
          3.0519731044769287,
          2.9748659133911133,
          3.0402116775512695,
          2.991548776626587,
          3.0478549003601074,
          2.975926160812378,
          3.016744613647461,
          2.9995288848876953,
          3.053551435470581,
          2.947509288787842,
          3.0018270015716553,
          3.0159194469451904,
          2.9683094024658203,
          3.0408594608306885,
          2.9947710037231445,
          3.019340753555298,
          2.9652016162872314,
          3.005938768386841,
          2.998560667037964,
          3.021381378173828,
          3.052271842956543,
          2.948700189590454,
          3.0704410076141357,
          3.049459934234619,
          2.919699192047119,
          3.0627481937408447,
          2.998699188232422,
          3.026510000228882,
          2.9912025928497314,
          3.0167789459228516,
          3.018890142440796,
          2.9499008655548096,
          2.973741054534912,
          3.058105707168579,
          3.044098377227783,
          3.0473601818084717,
          2.9640023708343506,
          3.02734375,
          3.0254271030426025,
          2.9808335304260254,
          2.9768640995025635,
          3.002413749694824,
          2.9648754596710205,
          3.0094523429870605,
          3.0196151733398438,
          3.065225839614868,
          2.976579189300537,
          2.9929306507110596,
          2.9731361865997314,
          2.964121103286743,
          2.981717348098755,
          2.950007677078247,
          2.9872493743896484,
          2.9937844276428223,
          2.9375689029693604,
          2.934276819229126,
          2.993868827819824,
          2.9669981002807617,
          3.0209829807281494,
          3.009129524230957,
          2.9791908264160156,
          3.025923252105713,
          2.976874589920044,
          2.995051860809326,
          2.947859764099121,
          2.9420976638793945,
          2.9499282836914062,
          2.953089952468872,
          2.9858951568603516,
          2.9256293773651123,
          2.9657812118530273,
          2.9649906158447266,
          2.9367148876190186,
          2.9242589473724365,
          2.9476842880249023,
          2.9035375118255615,
          2.9933629035949707,
          2.9779841899871826,
          2.9473447799682617,
          2.899872303009033,
          2.945566415786743,
          2.953540325164795,
          2.96482253074646,
          2.853245496749878,
          2.935370445251465,
          2.855567216873169,
          2.89593505859375,
          2.9210870265960693,
          3.056396961212158,
          2.88411283493042,
          2.8912508487701416,
          2.8945319652557373,
          2.9008045196533203,
          2.972327470779419,
          2.897862195968628,
          2.8232266902923584,
          2.8950140476226807,
          2.783867835998535,
          2.7969810962677,
          2.8176279067993164,
          2.734938621520996,
          2.86405611038208,
          2.7649624347686768,
          2.780604124069214,
          2.743631601333618,
          2.7986085414886475,
          2.715203046798706,
          2.7468442916870117,
          2.7324130535125732,
          2.7184064388275146,
          2.680671215057373,
          2.7081048488616943,
          2.7009692192077637,
          2.7049100399017334,
          2.6399877071380615,
          2.63529896736145,
          2.585280418395996,
          2.5911130905151367,
          2.599608898162842,
          2.6415350437164307,
          2.528817653656006,
          2.554206371307373,
          2.6459431648254395,
          2.5867562294006348,
          2.4908926486968994,
          2.6478826999664307,
          2.4899752140045166,
          2.473254680633545,
          2.584357976913452,
          2.5167176723480225,
          2.379396915435791,
          2.507977247238159,
          2.496492862701416,
          2.41861629486084,
          2.3812835216522217,
          2.3833811283111572,
          2.3053295612335205,
          2.3730669021606445,
          2.3107473850250244,
          2.133716344833374,
          2.3312809467315674,
          2.2398033142089844,
          2.370434522628784,
          2.2212510108947754,
          2.3307204246520996,
          2.164924144744873,
          2.146397113800049,
          2.095806121826172,
          2.083606004714966,
          2.088557481765747,
          2.039785385131836,
          2.0761988162994385,
          2.1164767742156982,
          2.1807491779327393,
          2.2800230979919434,
          2.1017913818359375,
          1.8041319847106934,
          1.9505959749221802,
          1.815487027168274,
          1.6919059753417969,
          1.8872379064559937,
          1.998763084411621,
          1.9889841079711914,
          1.7641167640686035,
          1.7673287391662598,
          1.83176589012146,
          1.5913749933242798,
          1.4114952087402344,
          1.8023314476013184,
          1.5114431381225586,
          1.7616411447525024,
          1.3689231872558594,
          1.387762427330017,
          1.5364083051681519,
          1.5821458101272583,
          1.4345121383666992,
          1.2735157012939453,
          1.4679083824157715,
          1.1155191659927368,
          1.6477198600769043,
          1.2864141464233398,
          1.7264349460601807,
          1.236566185951233,
          1.424409031867981,
          1.564747929573059,
          0.9231193661689758,
          1.1720380783081055,
          1.3079417943954468,
          0.9948471188545227,
          1.4860808849334717,
          1.5462026596069336,
          1.1625888347625732,
          1.3352471590042114,
          1.5638105869293213,
          1.2654329538345337,
          1.3775545358657837,
          1.1884591579437256,
          1.3717498779296875,
          1.2020469903945923,
          1.0815125703811646,
          1.0052943229675293,
          1.315783143043518,
          1.2639702558517456,
          1.0925185680389404,
          1.2602235078811646,
          1.745689868927002,
          0.825181782245636,
          1.5306884050369263,
          1.1704314947128296,
          1.2918751239776611,
          1.3483455181121826,
          1.1224546432495117,
          2.0331759452819824,
          1.540618658065796,
          0.6470741629600525,
          1.2001129388809204,
          1.3582252264022827,
          0.9913586974143982,
          1.18864905834198,
          1.0129088163375854,
          1.1769163608551025,
          1.1685773134231567,
          1.219224452972412,
          1.375531792640686,
          1.1840215921401978,
          0.8828390836715698,
          1.0467374324798584,
          1.0229995250701904,
          1.0509883165359497,
          1.2622874975204468,
          0.8048824667930603,
          1.034772276878357,
          1.0348303318023682,
          1.8919363021850586,
          1.1305967569351196,
          1.0910769701004028,
          0.8222469687461853,
          0.660982072353363,
          0.9101323485374451,
          1.570446252822876,
          1.0308170318603516,
          1.1032357215881348,
          0.7340252995491028,
          1.3572736978530884,
          0.8404683470726013
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "batch/loss"
        },
        "yaxis": {
         "hoverformat": ".6f"
        }
       }
      },
      "text/html": [
       "<div id=\"01670be5-2006-41c2-b548-d363e7d24c79\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"01670be5-2006-41c2-b548-d363e7d24c79\")) {\n",
       "    Plotly.newPlot(\"01670be5-2006-41c2-b548-d363e7d24c79\", [{\"name\": \"train/batch/loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260], \"y\": [3.0047085285186768, 2.9911835193634033, 3.0384418964385986, 3.077932834625244, 2.96917724609375, 3.024160146713257, 2.9735915660858154, 3.0246260166168213, 2.9587104320526123, 3.048039436340332, 3.040461778640747, 3.056180238723755, 3.0222675800323486, 3.060436487197876, 3.1094753742218018, 3.054736614227295, 3.0519731044769287, 2.9748659133911133, 3.0402116775512695, 2.991548776626587, 3.0478549003601074, 2.975926160812378, 3.016744613647461, 2.9995288848876953, 3.053551435470581, 2.947509288787842, 3.0018270015716553, 3.0159194469451904, 2.9683094024658203, 3.0408594608306885, 2.9947710037231445, 3.019340753555298, 2.9652016162872314, 3.005938768386841, 2.998560667037964, 3.021381378173828, 3.052271842956543, 2.948700189590454, 3.0704410076141357, 3.049459934234619, 2.919699192047119, 3.0627481937408447, 2.998699188232422, 3.026510000228882, 2.9912025928497314, 3.0167789459228516, 3.018890142440796, 2.9499008655548096, 2.973741054534912, 3.058105707168579, 3.044098377227783, 3.0473601818084717, 2.9640023708343506, 3.02734375, 3.0254271030426025, 2.9808335304260254, 2.9768640995025635, 3.002413749694824, 2.9648754596710205, 3.0094523429870605, 3.0196151733398438, 3.065225839614868, 2.976579189300537, 2.9929306507110596, 2.9731361865997314, 2.964121103286743, 2.981717348098755, 2.950007677078247, 2.9872493743896484, 2.9937844276428223, 2.9375689029693604, 2.934276819229126, 2.993868827819824, 2.9669981002807617, 3.0209829807281494, 3.009129524230957, 2.9791908264160156, 3.025923252105713, 2.976874589920044, 2.995051860809326, 2.947859764099121, 2.9420976638793945, 2.9499282836914062, 2.953089952468872, 2.9858951568603516, 2.9256293773651123, 2.9657812118530273, 2.9649906158447266, 2.9367148876190186, 2.9242589473724365, 2.9476842880249023, 2.9035375118255615, 2.9933629035949707, 2.9779841899871826, 2.9473447799682617, 2.899872303009033, 2.945566415786743, 2.953540325164795, 2.96482253074646, 2.853245496749878, 2.935370445251465, 2.855567216873169, 2.89593505859375, 2.9210870265960693, 3.056396961212158, 2.88411283493042, 2.8912508487701416, 2.8945319652557373, 2.9008045196533203, 2.972327470779419, 2.897862195968628, 2.8232266902923584, 2.8950140476226807, 2.783867835998535, 2.7969810962677, 2.8176279067993164, 2.734938621520996, 2.86405611038208, 2.7649624347686768, 2.780604124069214, 2.743631601333618, 2.7986085414886475, 2.715203046798706, 2.7468442916870117, 2.7324130535125732, 2.7184064388275146, 2.680671215057373, 2.7081048488616943, 2.7009692192077637, 2.7049100399017334, 2.6399877071380615, 2.63529896736145, 2.585280418395996, 2.5911130905151367, 2.599608898162842, 2.6415350437164307, 2.528817653656006, 2.554206371307373, 2.6459431648254395, 2.5867562294006348, 2.4908926486968994, 2.6478826999664307, 2.4899752140045166, 2.473254680633545, 2.584357976913452, 2.5167176723480225, 2.379396915435791, 2.507977247238159, 2.496492862701416, 2.41861629486084, 2.3812835216522217, 2.3833811283111572, 2.3053295612335205, 2.3730669021606445, 2.3107473850250244, 2.133716344833374, 2.3312809467315674, 2.2398033142089844, 2.370434522628784, 2.2212510108947754, 2.3307204246520996, 2.164924144744873, 2.146397113800049, 2.095806121826172, 2.083606004714966, 2.088557481765747, 2.039785385131836, 2.0761988162994385, 2.1164767742156982, 2.1807491779327393, 2.2800230979919434, 2.1017913818359375, 1.8041319847106934, 1.9505959749221802, 1.815487027168274, 1.6919059753417969, 1.8872379064559937, 1.998763084411621, 1.9889841079711914, 1.7641167640686035, 1.7673287391662598, 1.83176589012146, 1.5913749933242798, 1.4114952087402344, 1.8023314476013184, 1.5114431381225586, 1.7616411447525024, 1.3689231872558594, 1.387762427330017, 1.5364083051681519, 1.5821458101272583, 1.4345121383666992, 1.2735157012939453, 1.4679083824157715, 1.1155191659927368, 1.6477198600769043, 1.2864141464233398, 1.7264349460601807, 1.236566185951233, 1.424409031867981, 1.564747929573059, 0.9231193661689758, 1.1720380783081055, 1.3079417943954468, 0.9948471188545227, 1.4860808849334717, 1.5462026596069336, 1.1625888347625732, 1.3352471590042114, 1.5638105869293213, 1.2654329538345337, 1.3775545358657837, 1.1884591579437256, 1.3717498779296875, 1.2020469903945923, 1.0815125703811646, 1.0052943229675293, 1.315783143043518, 1.2639702558517456, 1.0925185680389404, 1.2602235078811646, 1.745689868927002, 0.825181782245636, 1.5306884050369263, 1.1704314947128296, 1.2918751239776611, 1.3483455181121826, 1.1224546432495117, 2.0331759452819824, 1.540618658065796, 0.6470741629600525, 1.2001129388809204, 1.3582252264022827, 0.9913586974143982, 1.18864905834198, 1.0129088163375854, 1.1769163608551025, 1.1685773134231567, 1.219224452972412, 1.375531792640686, 1.1840215921401978, 0.8828390836715698, 1.0467374324798584, 1.0229995250701904, 1.0509883165359497, 1.2622874975204468, 0.8048824667930603, 1.034772276878357, 1.0348303318023682, 1.8919363021850586, 1.1305967569351196, 1.0910769701004028, 0.8222469687461853, 0.660982072353363, 0.9101323485374451, 1.570446252822876, 1.0308170318603516, 1.1032357215881348, 0.7340252995491028, 1.3572736978530884, 0.8404683470726013], \"type\": \"scatter\", \"uid\": \"0dc00cca-6111-4a2a-9f4f-20b86cd83c04\"}], {\"title\": {\"text\": \"batch/loss\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"01670be5-2006-41c2-b548-d363e7d24c79\")) {window._Plotly.Plots.resize(document.getElementById(\"01670be5-2006-41c2-b548-d363e7d24c79\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"01670be5-2006-41c2-b548-d363e7d24c79\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"01670be5-2006-41c2-b548-d363e7d24c79\")) {\n",
       "    Plotly.newPlot(\"01670be5-2006-41c2-b548-d363e7d24c79\", [{\"name\": \"train/batch/loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260], \"y\": [3.0047085285186768, 2.9911835193634033, 3.0384418964385986, 3.077932834625244, 2.96917724609375, 3.024160146713257, 2.9735915660858154, 3.0246260166168213, 2.9587104320526123, 3.048039436340332, 3.040461778640747, 3.056180238723755, 3.0222675800323486, 3.060436487197876, 3.1094753742218018, 3.054736614227295, 3.0519731044769287, 2.9748659133911133, 3.0402116775512695, 2.991548776626587, 3.0478549003601074, 2.975926160812378, 3.016744613647461, 2.9995288848876953, 3.053551435470581, 2.947509288787842, 3.0018270015716553, 3.0159194469451904, 2.9683094024658203, 3.0408594608306885, 2.9947710037231445, 3.019340753555298, 2.9652016162872314, 3.005938768386841, 2.998560667037964, 3.021381378173828, 3.052271842956543, 2.948700189590454, 3.0704410076141357, 3.049459934234619, 2.919699192047119, 3.0627481937408447, 2.998699188232422, 3.026510000228882, 2.9912025928497314, 3.0167789459228516, 3.018890142440796, 2.9499008655548096, 2.973741054534912, 3.058105707168579, 3.044098377227783, 3.0473601818084717, 2.9640023708343506, 3.02734375, 3.0254271030426025, 2.9808335304260254, 2.9768640995025635, 3.002413749694824, 2.9648754596710205, 3.0094523429870605, 3.0196151733398438, 3.065225839614868, 2.976579189300537, 2.9929306507110596, 2.9731361865997314, 2.964121103286743, 2.981717348098755, 2.950007677078247, 2.9872493743896484, 2.9937844276428223, 2.9375689029693604, 2.934276819229126, 2.993868827819824, 2.9669981002807617, 3.0209829807281494, 3.009129524230957, 2.9791908264160156, 3.025923252105713, 2.976874589920044, 2.995051860809326, 2.947859764099121, 2.9420976638793945, 2.9499282836914062, 2.953089952468872, 2.9858951568603516, 2.9256293773651123, 2.9657812118530273, 2.9649906158447266, 2.9367148876190186, 2.9242589473724365, 2.9476842880249023, 2.9035375118255615, 2.9933629035949707, 2.9779841899871826, 2.9473447799682617, 2.899872303009033, 2.945566415786743, 2.953540325164795, 2.96482253074646, 2.853245496749878, 2.935370445251465, 2.855567216873169, 2.89593505859375, 2.9210870265960693, 3.056396961212158, 2.88411283493042, 2.8912508487701416, 2.8945319652557373, 2.9008045196533203, 2.972327470779419, 2.897862195968628, 2.8232266902923584, 2.8950140476226807, 2.783867835998535, 2.7969810962677, 2.8176279067993164, 2.734938621520996, 2.86405611038208, 2.7649624347686768, 2.780604124069214, 2.743631601333618, 2.7986085414886475, 2.715203046798706, 2.7468442916870117, 2.7324130535125732, 2.7184064388275146, 2.680671215057373, 2.7081048488616943, 2.7009692192077637, 2.7049100399017334, 2.6399877071380615, 2.63529896736145, 2.585280418395996, 2.5911130905151367, 2.599608898162842, 2.6415350437164307, 2.528817653656006, 2.554206371307373, 2.6459431648254395, 2.5867562294006348, 2.4908926486968994, 2.6478826999664307, 2.4899752140045166, 2.473254680633545, 2.584357976913452, 2.5167176723480225, 2.379396915435791, 2.507977247238159, 2.496492862701416, 2.41861629486084, 2.3812835216522217, 2.3833811283111572, 2.3053295612335205, 2.3730669021606445, 2.3107473850250244, 2.133716344833374, 2.3312809467315674, 2.2398033142089844, 2.370434522628784, 2.2212510108947754, 2.3307204246520996, 2.164924144744873, 2.146397113800049, 2.095806121826172, 2.083606004714966, 2.088557481765747, 2.039785385131836, 2.0761988162994385, 2.1164767742156982, 2.1807491779327393, 2.2800230979919434, 2.1017913818359375, 1.8041319847106934, 1.9505959749221802, 1.815487027168274, 1.6919059753417969, 1.8872379064559937, 1.998763084411621, 1.9889841079711914, 1.7641167640686035, 1.7673287391662598, 1.83176589012146, 1.5913749933242798, 1.4114952087402344, 1.8023314476013184, 1.5114431381225586, 1.7616411447525024, 1.3689231872558594, 1.387762427330017, 1.5364083051681519, 1.5821458101272583, 1.4345121383666992, 1.2735157012939453, 1.4679083824157715, 1.1155191659927368, 1.6477198600769043, 1.2864141464233398, 1.7264349460601807, 1.236566185951233, 1.424409031867981, 1.564747929573059, 0.9231193661689758, 1.1720380783081055, 1.3079417943954468, 0.9948471188545227, 1.4860808849334717, 1.5462026596069336, 1.1625888347625732, 1.3352471590042114, 1.5638105869293213, 1.2654329538345337, 1.3775545358657837, 1.1884591579437256, 1.3717498779296875, 1.2020469903945923, 1.0815125703811646, 1.0052943229675293, 1.315783143043518, 1.2639702558517456, 1.0925185680389404, 1.2602235078811646, 1.745689868927002, 0.825181782245636, 1.5306884050369263, 1.1704314947128296, 1.2918751239776611, 1.3483455181121826, 1.1224546432495117, 2.0331759452819824, 1.540618658065796, 0.6470741629600525, 1.2001129388809204, 1.3582252264022827, 0.9913586974143982, 1.18864905834198, 1.0129088163375854, 1.1769163608551025, 1.1685773134231567, 1.219224452972412, 1.375531792640686, 1.1840215921401978, 0.8828390836715698, 1.0467374324798584, 1.0229995250701904, 1.0509883165359497, 1.2622874975204468, 0.8048824667930603, 1.034772276878357, 1.0348303318023682, 1.8919363021850586, 1.1305967569351196, 1.0910769701004028, 0.8222469687461853, 0.660982072353363, 0.9101323485374451, 1.570446252822876, 1.0308170318603516, 1.1032357215881348, 0.7340252995491028, 1.3572736978530884, 0.8404683470726013], \"type\": \"scatter\", \"uid\": \"0dc00cca-6111-4a2a-9f4f-20b86cd83c04\"}], {\"title\": {\"text\": \"batch/loss\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"01670be5-2006-41c2-b548-d363e7d24c79\")) {window._Plotly.Plots.resize(document.getElementById(\"01670be5-2006-41c2-b548-d363e7d24c79\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "name": "train/batch/lr",
         "type": "scatter",
         "uid": "7e7b073c-3553-4470-b0c2-6b7b4b873899",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260
         ],
         "y": [
          9.999999974752427e-07,
          1.0452754395373631e-06,
          1.0926008826572797e-06,
          1.142068867920898e-06,
          1.1937765975744696e-06,
          1.2478254802772426e-06,
          1.3043213584751356e-06,
          1.363375190521765e-06,
          1.4251027096179314e-06,
          1.4896248785589705e-06,
          1.557068458168942e-06,
          1.6275654388664407e-06,
          1.7012542912198114e-06,
          1.7782793975129607e-06,
          1.8587918475532206e-06,
          1.942949666045024e-06,
          2.0309175852162298e-06,
          2.1228684090601746e-06,
          2.218982444901485e-06,
          2.319447958143428e-06,
          2.424462081762613e-06,
          2.534230816308991e-06,
          2.648969257279532e-06,
          2.7689027319866e-06,
          2.8942661174369277e-06,
          3.025305431947345e-06,
          3.1622776077711023e-06,
          3.305451400592574e-06,
          3.4551073895272566e-06,
          3.611538886616472e-06,
          3.77505330106942e-06,
          3.945970547647448e-06,
          4.124626229895512e-06,
          4.3113709580211435e-06,
          4.506570348894456e-06,
          4.710607754532248e-06,
          4.923882443108596e-06,
          5.146813691681018e-06,
          5.379838512453716e-06,
          5.62341347176698e-06,
          5.878016054339241e-06,
          6.144146027509123e-06,
          6.422325441235444e-06,
          6.713099537591916e-06,
          7.0170381150092e-06,
          7.33473825675901e-06,
          7.66682205721736e-06,
          8.013941624085419e-06,
          8.376776349905413e-06,
          8.75603927852353e-06,
          9.152473467111122e-06,
          9.5668556241435e-06,
          9.999999747378752e-06,
          1.0452755304868333e-05,
          1.0926009053946473e-05,
          1.1420688679208979e-05,
          1.1937766430492047e-05,
          1.247825457539875e-05,
          1.3043213584751356e-05,
          1.3633752132591326e-05,
          1.4251027096179314e-05,
          1.489624901296338e-05,
          1.5570683899568394e-05,
          1.6275655070785433e-05,
          1.7012542230077088e-05,
          1.778279329300858e-05,
          1.8587918020784855e-05,
          1.9429495296208188e-05,
          2.0309176761657e-05,
          2.1228685000096448e-05,
          2.2189822630025446e-05,
          2.3194477762444876e-05,
          2.4244620362878777e-05,
          2.534230770834256e-05,
          2.6489693482290022e-05,
          2.7689025955623947e-05,
          2.8942660719621927e-05,
          3.025305522896815e-05,
          3.162277789670043e-05,
          3.3054511732188985e-05,
          3.455107435001992e-05,
          3.6115390685154125e-05,
          3.7750531191704795e-05,
          3.945970456697978e-05,
          4.124626502743922e-05,
          4.311370867071673e-05,
          4.506570257944986e-05,
          4.710607390734367e-05,
          4.9238828069064766e-05,
          5.146813782630488e-05,
          5.379838330554776e-05,
          5.62341338081751e-05,
          5.8780162362381816e-05,
          6.144146027509123e-05,
          6.422325532184914e-05,
          6.713099719490856e-05,
          7.01703829690814e-05,
          7.33473789296113e-05,
          7.66682205721736e-05,
          8.013941260287538e-05,
          8.376776531804353e-05,
          8.756039460422471e-05,
          9.152472921414301e-05,
          9.566856169840321e-05,
          9.999999747378752e-05,
          0.00010452754941070452,
          0.00010926008690148592,
          0.0001142068940680474,
          0.00011937766248593107,
          0.0001247825421160087,
          0.00013043213402852416,
          0.00013633751950692385,
          0.00014251026732381433,
          0.0001489624846726656,
          0.00015570684627164155,
          0.00016275655070785433,
          0.00017012542230077088,
          0.00017782794020604342,
          0.00018587919475976378,
          0.0001942949602380395,
          0.00020309176761657,
          0.00021228684636298567,
          0.0002218982408521697,
          0.00023194478126242757,
          0.00024244619999080896,
          0.000253423087997362,
          0.0002648969239089638,
          0.0002768902631942183,
          0.0002894266217481345,
          0.0003025305340997875,
          0.0003162277571391314,
          0.00033054512459784746,
          0.0003455107216723263,
          0.0003611538850236684,
          0.00037750531919300556,
          0.00039459706749767065,
          0.0004124626284465194,
          0.00043113710125908256,
          0.00045065704034641385,
          0.0004710607463493943,
          0.0004923882661387324,
          0.0005146813928149641,
          0.0005379838403314352,
          0.000562341301701963,
          0.0005878016236238182,
          0.0006144146318547428,
          0.0006422325386665761,
          0.0006713099428452551,
          0.000701703829690814,
          0.0007334738038480282,
          0.000766682205721736,
          0.0008013941114768386,
          0.0008376776240766048,
          0.0008756039314903319,
          0.0009152473066933453,
          0.0009566855733282864,
          0.0010000000474974513,
          0.00104527547955513,
          0.0010926008690148592,
          0.0011420688824728131,
          0.0011937766103073955,
          0.0012478254502639174,
          0.0013043213402852416,
          0.001363375224173069,
          0.0014251027023419738,
          0.001489624846726656,
          0.001557068433612585,
          0.0016275654779747128,
          0.0017012542812153697,
          0.0017782794311642647,
          0.0018587919184938073,
          0.001942949602380395,
          0.0020309176761657,
          0.0021228683181107044,
          0.0022189824376255274,
          0.0023194479290395975,
          0.0024244619999080896,
          0.0025342307053506374,
          0.002648969180881977,
          0.002768902573734522,
          0.002894266042858362,
          0.0030253054574131966,
          0.003162277629598975,
          0.0033054512459784746,
          0.003455107333138585,
          0.003611539024859667,
          0.0037750531919300556,
          0.003945970442146063,
          0.004124626517295837,
          0.00431137066334486,
          0.004506570287048817,
          0.004710607696324587,
          0.004923882428556681,
          0.005146814044564962,
          0.005379838403314352,
          0.005623413249850273,
          0.005878015886992216,
          0.006144145969301462,
          0.006422325503081083,
          0.0067130993120372295,
          0.00701703829690814,
          0.007334738038480282,
          0.0076668220572173595,
          0.00801394134759903,
          0.008376776240766048,
          0.00875603873282671,
          0.009152472950518131,
          0.009566855616867542,
          0.009999999776482582,
          0.0104527547955513,
          0.010926008224487305,
          0.011420689523220062,
          0.011937766335904598,
          0.012478254735469818,
          0.01304321363568306,
          0.01363375224173069,
          0.014251026324927807,
          0.014896249398589134,
          0.01557068433612585,
          0.01627565361559391,
          0.017012542113661766,
          0.017782794311642647,
          0.01858791895210743,
          0.019429495558142662,
          0.020309176295995712,
          0.02122868411242962,
          0.022189823910593987,
          0.023194478824734688,
          0.02424461953341961,
          0.02534230798482895,
          0.02648969367146492,
          0.027689026668667793,
          0.028942661359906197,
          0.030253054574131966,
          0.03162277489900589,
          0.03305451199412346,
          0.034551072865724564,
          0.036115389317274094,
          0.03775053098797798,
          0.0394597053527832,
          0.041246265172958374,
          0.04311370849609375,
          0.045065704733133316,
          0.047106076031923294,
          0.04923882707953453,
          0.0514681376516819,
          0.053798384964466095,
          0.05623413249850273,
          0.058780159801244736,
          0.061441462486982346,
          0.06422325223684311,
          0.06713099032640457,
          0.07017038017511368,
          0.07334738224744797,
          0.07666821777820587,
          0.0801394134759903,
          0.08376776427030563,
          0.08756039291620255,
          0.09152472764253616,
          0.09566856175661087,
          0.10000000149011612
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "batch/lr"
        },
        "yaxis": {
         "hoverformat": ".6f"
        }
       }
      },
      "text/html": [
       "<div id=\"9bfa060e-574d-4197-a543-264bd8646647\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"9bfa060e-574d-4197-a543-264bd8646647\")) {\n",
       "    Plotly.newPlot(\"9bfa060e-574d-4197-a543-264bd8646647\", [{\"name\": \"train/batch/lr\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260], \"y\": [9.999999974752427e-07, 1.0452754395373631e-06, 1.0926008826572797e-06, 1.142068867920898e-06, 1.1937765975744696e-06, 1.2478254802772426e-06, 1.3043213584751356e-06, 1.363375190521765e-06, 1.4251027096179314e-06, 1.4896248785589705e-06, 1.557068458168942e-06, 1.6275654388664407e-06, 1.7012542912198114e-06, 1.7782793975129607e-06, 1.8587918475532206e-06, 1.942949666045024e-06, 2.0309175852162298e-06, 2.1228684090601746e-06, 2.218982444901485e-06, 2.319447958143428e-06, 2.424462081762613e-06, 2.534230816308991e-06, 2.648969257279532e-06, 2.7689027319866e-06, 2.8942661174369277e-06, 3.025305431947345e-06, 3.1622776077711023e-06, 3.305451400592574e-06, 3.4551073895272566e-06, 3.611538886616472e-06, 3.77505330106942e-06, 3.945970547647448e-06, 4.124626229895512e-06, 4.3113709580211435e-06, 4.506570348894456e-06, 4.710607754532248e-06, 4.923882443108596e-06, 5.146813691681018e-06, 5.379838512453716e-06, 5.62341347176698e-06, 5.878016054339241e-06, 6.144146027509123e-06, 6.422325441235444e-06, 6.713099537591916e-06, 7.0170381150092e-06, 7.33473825675901e-06, 7.66682205721736e-06, 8.013941624085419e-06, 8.376776349905413e-06, 8.75603927852353e-06, 9.152473467111122e-06, 9.5668556241435e-06, 9.999999747378752e-06, 1.0452755304868333e-05, 1.0926009053946473e-05, 1.1420688679208979e-05, 1.1937766430492047e-05, 1.247825457539875e-05, 1.3043213584751356e-05, 1.3633752132591326e-05, 1.4251027096179314e-05, 1.489624901296338e-05, 1.5570683899568394e-05, 1.6275655070785433e-05, 1.7012542230077088e-05, 1.778279329300858e-05, 1.8587918020784855e-05, 1.9429495296208188e-05, 2.0309176761657e-05, 2.1228685000096448e-05, 2.2189822630025446e-05, 2.3194477762444876e-05, 2.4244620362878777e-05, 2.534230770834256e-05, 2.6489693482290022e-05, 2.7689025955623947e-05, 2.8942660719621927e-05, 3.025305522896815e-05, 3.162277789670043e-05, 3.3054511732188985e-05, 3.455107435001992e-05, 3.6115390685154125e-05, 3.7750531191704795e-05, 3.945970456697978e-05, 4.124626502743922e-05, 4.311370867071673e-05, 4.506570257944986e-05, 4.710607390734367e-05, 4.9238828069064766e-05, 5.146813782630488e-05, 5.379838330554776e-05, 5.62341338081751e-05, 5.8780162362381816e-05, 6.144146027509123e-05, 6.422325532184914e-05, 6.713099719490856e-05, 7.01703829690814e-05, 7.33473789296113e-05, 7.66682205721736e-05, 8.013941260287538e-05, 8.376776531804353e-05, 8.756039460422471e-05, 9.152472921414301e-05, 9.566856169840321e-05, 9.999999747378752e-05, 0.00010452754941070452, 0.00010926008690148592, 0.0001142068940680474, 0.00011937766248593107, 0.0001247825421160087, 0.00013043213402852416, 0.00013633751950692385, 0.00014251026732381433, 0.0001489624846726656, 0.00015570684627164155, 0.00016275655070785433, 0.00017012542230077088, 0.00017782794020604342, 0.00018587919475976378, 0.0001942949602380395, 0.00020309176761657, 0.00021228684636298567, 0.0002218982408521697, 0.00023194478126242757, 0.00024244619999080896, 0.000253423087997362, 0.0002648969239089638, 0.0002768902631942183, 0.0002894266217481345, 0.0003025305340997875, 0.0003162277571391314, 0.00033054512459784746, 0.0003455107216723263, 0.0003611538850236684, 0.00037750531919300556, 0.00039459706749767065, 0.0004124626284465194, 0.00043113710125908256, 0.00045065704034641385, 0.0004710607463493943, 0.0004923882661387324, 0.0005146813928149641, 0.0005379838403314352, 0.000562341301701963, 0.0005878016236238182, 0.0006144146318547428, 0.0006422325386665761, 0.0006713099428452551, 0.000701703829690814, 0.0007334738038480282, 0.000766682205721736, 0.0008013941114768386, 0.0008376776240766048, 0.0008756039314903319, 0.0009152473066933453, 0.0009566855733282864, 0.0010000000474974513, 0.00104527547955513, 0.0010926008690148592, 0.0011420688824728131, 0.0011937766103073955, 0.0012478254502639174, 0.0013043213402852416, 0.001363375224173069, 0.0014251027023419738, 0.001489624846726656, 0.001557068433612585, 0.0016275654779747128, 0.0017012542812153697, 0.0017782794311642647, 0.0018587919184938073, 0.001942949602380395, 0.0020309176761657, 0.0021228683181107044, 0.0022189824376255274, 0.0023194479290395975, 0.0024244619999080896, 0.0025342307053506374, 0.002648969180881977, 0.002768902573734522, 0.002894266042858362, 0.0030253054574131966, 0.003162277629598975, 0.0033054512459784746, 0.003455107333138585, 0.003611539024859667, 0.0037750531919300556, 0.003945970442146063, 0.004124626517295837, 0.00431137066334486, 0.004506570287048817, 0.004710607696324587, 0.004923882428556681, 0.005146814044564962, 0.005379838403314352, 0.005623413249850273, 0.005878015886992216, 0.006144145969301462, 0.006422325503081083, 0.0067130993120372295, 0.00701703829690814, 0.007334738038480282, 0.0076668220572173595, 0.00801394134759903, 0.008376776240766048, 0.00875603873282671, 0.009152472950518131, 0.009566855616867542, 0.009999999776482582, 0.0104527547955513, 0.010926008224487305, 0.011420689523220062, 0.011937766335904598, 0.012478254735469818, 0.01304321363568306, 0.01363375224173069, 0.014251026324927807, 0.014896249398589134, 0.01557068433612585, 0.01627565361559391, 0.017012542113661766, 0.017782794311642647, 0.01858791895210743, 0.019429495558142662, 0.020309176295995712, 0.02122868411242962, 0.022189823910593987, 0.023194478824734688, 0.02424461953341961, 0.02534230798482895, 0.02648969367146492, 0.027689026668667793, 0.028942661359906197, 0.030253054574131966, 0.03162277489900589, 0.03305451199412346, 0.034551072865724564, 0.036115389317274094, 0.03775053098797798, 0.0394597053527832, 0.041246265172958374, 0.04311370849609375, 0.045065704733133316, 0.047106076031923294, 0.04923882707953453, 0.0514681376516819, 0.053798384964466095, 0.05623413249850273, 0.058780159801244736, 0.061441462486982346, 0.06422325223684311, 0.06713099032640457, 0.07017038017511368, 0.07334738224744797, 0.07666821777820587, 0.0801394134759903, 0.08376776427030563, 0.08756039291620255, 0.09152472764253616, 0.09566856175661087, 0.10000000149011612], \"type\": \"scatter\", \"uid\": \"7e7b073c-3553-4470-b0c2-6b7b4b873899\"}], {\"title\": {\"text\": \"batch/lr\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"9bfa060e-574d-4197-a543-264bd8646647\")) {window._Plotly.Plots.resize(document.getElementById(\"9bfa060e-574d-4197-a543-264bd8646647\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"9bfa060e-574d-4197-a543-264bd8646647\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"9bfa060e-574d-4197-a543-264bd8646647\")) {\n",
       "    Plotly.newPlot(\"9bfa060e-574d-4197-a543-264bd8646647\", [{\"name\": \"train/batch/lr\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260], \"y\": [9.999999974752427e-07, 1.0452754395373631e-06, 1.0926008826572797e-06, 1.142068867920898e-06, 1.1937765975744696e-06, 1.2478254802772426e-06, 1.3043213584751356e-06, 1.363375190521765e-06, 1.4251027096179314e-06, 1.4896248785589705e-06, 1.557068458168942e-06, 1.6275654388664407e-06, 1.7012542912198114e-06, 1.7782793975129607e-06, 1.8587918475532206e-06, 1.942949666045024e-06, 2.0309175852162298e-06, 2.1228684090601746e-06, 2.218982444901485e-06, 2.319447958143428e-06, 2.424462081762613e-06, 2.534230816308991e-06, 2.648969257279532e-06, 2.7689027319866e-06, 2.8942661174369277e-06, 3.025305431947345e-06, 3.1622776077711023e-06, 3.305451400592574e-06, 3.4551073895272566e-06, 3.611538886616472e-06, 3.77505330106942e-06, 3.945970547647448e-06, 4.124626229895512e-06, 4.3113709580211435e-06, 4.506570348894456e-06, 4.710607754532248e-06, 4.923882443108596e-06, 5.146813691681018e-06, 5.379838512453716e-06, 5.62341347176698e-06, 5.878016054339241e-06, 6.144146027509123e-06, 6.422325441235444e-06, 6.713099537591916e-06, 7.0170381150092e-06, 7.33473825675901e-06, 7.66682205721736e-06, 8.013941624085419e-06, 8.376776349905413e-06, 8.75603927852353e-06, 9.152473467111122e-06, 9.5668556241435e-06, 9.999999747378752e-06, 1.0452755304868333e-05, 1.0926009053946473e-05, 1.1420688679208979e-05, 1.1937766430492047e-05, 1.247825457539875e-05, 1.3043213584751356e-05, 1.3633752132591326e-05, 1.4251027096179314e-05, 1.489624901296338e-05, 1.5570683899568394e-05, 1.6275655070785433e-05, 1.7012542230077088e-05, 1.778279329300858e-05, 1.8587918020784855e-05, 1.9429495296208188e-05, 2.0309176761657e-05, 2.1228685000096448e-05, 2.2189822630025446e-05, 2.3194477762444876e-05, 2.4244620362878777e-05, 2.534230770834256e-05, 2.6489693482290022e-05, 2.7689025955623947e-05, 2.8942660719621927e-05, 3.025305522896815e-05, 3.162277789670043e-05, 3.3054511732188985e-05, 3.455107435001992e-05, 3.6115390685154125e-05, 3.7750531191704795e-05, 3.945970456697978e-05, 4.124626502743922e-05, 4.311370867071673e-05, 4.506570257944986e-05, 4.710607390734367e-05, 4.9238828069064766e-05, 5.146813782630488e-05, 5.379838330554776e-05, 5.62341338081751e-05, 5.8780162362381816e-05, 6.144146027509123e-05, 6.422325532184914e-05, 6.713099719490856e-05, 7.01703829690814e-05, 7.33473789296113e-05, 7.66682205721736e-05, 8.013941260287538e-05, 8.376776531804353e-05, 8.756039460422471e-05, 9.152472921414301e-05, 9.566856169840321e-05, 9.999999747378752e-05, 0.00010452754941070452, 0.00010926008690148592, 0.0001142068940680474, 0.00011937766248593107, 0.0001247825421160087, 0.00013043213402852416, 0.00013633751950692385, 0.00014251026732381433, 0.0001489624846726656, 0.00015570684627164155, 0.00016275655070785433, 0.00017012542230077088, 0.00017782794020604342, 0.00018587919475976378, 0.0001942949602380395, 0.00020309176761657, 0.00021228684636298567, 0.0002218982408521697, 0.00023194478126242757, 0.00024244619999080896, 0.000253423087997362, 0.0002648969239089638, 0.0002768902631942183, 0.0002894266217481345, 0.0003025305340997875, 0.0003162277571391314, 0.00033054512459784746, 0.0003455107216723263, 0.0003611538850236684, 0.00037750531919300556, 0.00039459706749767065, 0.0004124626284465194, 0.00043113710125908256, 0.00045065704034641385, 0.0004710607463493943, 0.0004923882661387324, 0.0005146813928149641, 0.0005379838403314352, 0.000562341301701963, 0.0005878016236238182, 0.0006144146318547428, 0.0006422325386665761, 0.0006713099428452551, 0.000701703829690814, 0.0007334738038480282, 0.000766682205721736, 0.0008013941114768386, 0.0008376776240766048, 0.0008756039314903319, 0.0009152473066933453, 0.0009566855733282864, 0.0010000000474974513, 0.00104527547955513, 0.0010926008690148592, 0.0011420688824728131, 0.0011937766103073955, 0.0012478254502639174, 0.0013043213402852416, 0.001363375224173069, 0.0014251027023419738, 0.001489624846726656, 0.001557068433612585, 0.0016275654779747128, 0.0017012542812153697, 0.0017782794311642647, 0.0018587919184938073, 0.001942949602380395, 0.0020309176761657, 0.0021228683181107044, 0.0022189824376255274, 0.0023194479290395975, 0.0024244619999080896, 0.0025342307053506374, 0.002648969180881977, 0.002768902573734522, 0.002894266042858362, 0.0030253054574131966, 0.003162277629598975, 0.0033054512459784746, 0.003455107333138585, 0.003611539024859667, 0.0037750531919300556, 0.003945970442146063, 0.004124626517295837, 0.00431137066334486, 0.004506570287048817, 0.004710607696324587, 0.004923882428556681, 0.005146814044564962, 0.005379838403314352, 0.005623413249850273, 0.005878015886992216, 0.006144145969301462, 0.006422325503081083, 0.0067130993120372295, 0.00701703829690814, 0.007334738038480282, 0.0076668220572173595, 0.00801394134759903, 0.008376776240766048, 0.00875603873282671, 0.009152472950518131, 0.009566855616867542, 0.009999999776482582, 0.0104527547955513, 0.010926008224487305, 0.011420689523220062, 0.011937766335904598, 0.012478254735469818, 0.01304321363568306, 0.01363375224173069, 0.014251026324927807, 0.014896249398589134, 0.01557068433612585, 0.01627565361559391, 0.017012542113661766, 0.017782794311642647, 0.01858791895210743, 0.019429495558142662, 0.020309176295995712, 0.02122868411242962, 0.022189823910593987, 0.023194478824734688, 0.02424461953341961, 0.02534230798482895, 0.02648969367146492, 0.027689026668667793, 0.028942661359906197, 0.030253054574131966, 0.03162277489900589, 0.03305451199412346, 0.034551072865724564, 0.036115389317274094, 0.03775053098797798, 0.0394597053527832, 0.041246265172958374, 0.04311370849609375, 0.045065704733133316, 0.047106076031923294, 0.04923882707953453, 0.0514681376516819, 0.053798384964466095, 0.05623413249850273, 0.058780159801244736, 0.061441462486982346, 0.06422325223684311, 0.06713099032640457, 0.07017038017511368, 0.07334738224744797, 0.07666821777820587, 0.0801394134759903, 0.08376776427030563, 0.08756039291620255, 0.09152472764253616, 0.09566856175661087, 0.10000000149011612], \"type\": \"scatter\", \"uid\": \"7e7b073c-3553-4470-b0c2-6b7b4b873899\"}], {\"title\": {\"text\": \"batch/lr\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"9bfa060e-574d-4197-a543-264bd8646647\")) {window._Plotly.Plots.resize(document.getElementById(\"9bfa060e-574d-4197-a543-264bd8646647\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keker.plot_kek_lr(logdir=\"logdir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100% 261/261 [03:38<00:00,  1.19it/s, loss=2.8771, val_loss=2.8088]\n",
      "Epoch 2/3:   7% 17/261 [00:16<03:09,  1.28it/s, loss=2.8607]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-5776855ce097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ok, now let's start training!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# It's as simple as:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mkeker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/aseg_torch1/lib/python3.6/site-packages/kekas/keker.py\u001b[0m in \u001b[0;36mkek\u001b[0;34m(self, lr, epochs, skip_val, opt, opt_params, sched, sched_params, stop_iter, logdir, cp_saver_params, early_stop_params)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/aseg_torch1/lib/python3.6/site-packages/kekas/keker.py\u001b[0m in \u001b[0;36m_run_epoch\u001b[0;34m(self, epoch, epochs)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/aseg_torch1/lib/python3.6/site-packages/kekas/keker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[1;32m    474\u001b[0m         preds = self.step_fn(model=self.state.core.model,\n\u001b[0;32m--> 475\u001b[0;31m                              batch=self.state.core.batch)\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-178e21faa00f>\u001b[0m in \u001b[0;36mstep_fn\u001b[0;34m(model, batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# you could define here whatever logic you want\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m  \u001b[0;31m# here we get an \"image\" from our dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/aseg_torch1/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-dbddd0ec0de1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xb)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coords'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparseModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/aseg_torch1/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/aseg_torch1/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/aseg_torch1/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Linux_3Tb/a/SparseConvNet_aseg/sparseconvnet/submanifoldConvolution.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             self.filter_size)\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Linux_3Tb/a/SparseConvNet_aseg/sparseconvnet/submanifoldConvolution.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input_features, weight, bias, input_metadata, spatial_size, dimension, filter_size)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0moutput_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 bias)\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0msparseconvnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moutput_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ok, now let's start training!\n",
    "# It's as simple as:\n",
    "keker.kek(lr=1e-5, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from kekas Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe creation and train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'PetImages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-9657eaa37c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# some files there are corrupted, so add only good ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                 \u001b[0;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(pathobj, *args)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PetImages'"
     ]
    }
   ],
   "source": [
    "# Let's create a pandas DataFrame to help us with data handling\n",
    "root_dir = Path(\"PetImages/\")  # path to Cats and Dogs dataset root directory\n",
    "\n",
    "fpaths = []\n",
    "labels = []\n",
    "for d in root_dir.iterdir():\n",
    "    for f in d.iterdir():\n",
    "        img = cv2.imread(str(f))  # some files there are corrupted, so add only good ones\n",
    "        if img is not None:\n",
    "            labels.append(d.name)\n",
    "            fpaths.append(str(f))\n",
    "\n",
    "df = pd.DataFrame(data={\"fpath\": fpaths, \"label\": labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split dataset to train and val parts\n",
    "train_df, val_df = train_test_split(df, test_size=2000)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and val datasets using DataKek class - a pytorch Dataset that uses pandas DataFrame as data source\n",
    "\n",
    "# at first we need to create a reader function that will define how image will be opened\n",
    "def reader_fn(i, row):\n",
    "    # it always gets i and row as parameters\n",
    "    # where i is an index of dataframe and row is a dataframes row\n",
    "    image = cv2.imread(row[\"fpath\"])[:,:,::-1]  # BGR -> RGB\n",
    "    if row[\"label\"] == \"Dog\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return {\"image\": image, \"label\": label}\n",
    "\n",
    "\n",
    "# Then we should create transformations/augmentations\n",
    "# We will use awesome https://github.com/albu/albumentations library\n",
    "def augs(p=0.5):\n",
    "    return Compose([\n",
    "        CLAHE(),\n",
    "        RandomRotate90(),\n",
    "        Transpose(),\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n",
    "        Blur(blur_limit=3),\n",
    "        OpticalDistortion(),\n",
    "        GridDistortion(),\n",
    "        HueSaturationValue()\n",
    "    ], p=p)\n",
    "\n",
    "def get_transforms(dataset_key, size, p):\n",
    "    # we need to use a Transformer class to apply transformations to DataKeks elements\n",
    "    # dataset_key is an image key in dict returned by reader_fn\n",
    "    \n",
    "    PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n",
    "\n",
    "    AUGS = Transformer(dataset_key, lambda x: augs()(image=x)[\"image\"])\n",
    "\n",
    "    NRM_TFMS = transforms.Compose([\n",
    "        Transformer(dataset_key, to_torch()),\n",
    "        Transformer(dataset_key, normalize())\n",
    "    ])\n",
    "    \n",
    "    train_tfms = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n",
    "    val_tfms = transforms.Compose([PRE_TFMS, NRM_TFMS])  # because we don't want to augment val set yet\n",
    "    \n",
    "    return train_tfms, val_tfms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataKeks creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create DataKeks\n",
    "train_tfms, val_tfms = get_transforms(\"image\", 224, 0.5)\n",
    "\n",
    "train_dk = DataKek(df=train_df, reader_fn=reader_fn, transforms=train_tfms)\n",
    "val_dk = DataKek(df=val_df, reader_fn=reader_fn, transforms=val_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and DataLoaders\n",
    "batch_size = 32\n",
    "workers = 8\n",
    "\n",
    "train_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=workers, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple neural network using pretrainedmodels library\n",
    "# https://github.com/Cadene/pretrained-models.pytorch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes: int,\n",
    "            p: float = 0.5,\n",
    "            pooling_size: int = 2,\n",
    "            last_conv_size: int = 2048,\n",
    "            arch: str = \"se_resnext50_32x4d\",\n",
    "            pretrained: str = \"imagenet\") -> None:\n",
    "        \"\"\"A simple model to finetune.\n",
    "        \n",
    "        Args:\n",
    "            num_classes: the number of target classes, the size of the last layer's output\n",
    "            p: dropout probability\n",
    "            pooling_size: the size of the result feature map after adaptive pooling layer\n",
    "            last_conv_size: size of the flatten last backbone conv layer\n",
    "            arch: the name of the architecture form pretrainedmodels\n",
    "            pretrained: the mode for pretrained model from pretrainedmodels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        net = pm.__dict__[arch](pretrained=pretrained)\n",
    "        modules = list(net.children())[:-2]  # delete last layers: pooling and linear\n",
    "        \n",
    "        # add custom head\n",
    "        modules += [nn.Sequential(\n",
    "            # AdaptiveConcatPool2d is a concat of AdaptiveMaxPooling and AdaptiveAveragePooling \n",
    "            AdaptiveConcatPool2d(size=pooling_size),\n",
    "            Flatten(),\n",
    "            nn.BatchNorm1d(2 * pooling_size * pooling_size * last_conv_size),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(2 * pooling_size * pooling_size * last_conv_size, num_classes)\n",
    "        )]\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the three whales of your pipelane are: the data, the model and the loss (hi, Jeremy)\n",
    "\n",
    "# the data is represented in Kekas by DataOwner. It is a namedtuple with three fields:\n",
    "# 'train_dl', 'val_dl', 'test_dl'\n",
    "# For training process we will need at least two of them, and we can skip 'test_dl' for now\n",
    "# so we will initialize it with `None` value.\n",
    "dataowner = DataOwner(train_dl, val_dl, None)\n",
    "\n",
    "# model is just a pytorch nn.Module, that we created vefore\n",
    "model = Net(num_classes=2)\n",
    "\n",
    "# loss or criterion is also a pytorch nn.Module. For multiloss scenarios it can be a list of nn.Modules\n",
    "# for our simple example let's use the standart cross entopy criterion\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also we need to specify, what model will do with each batch of data on each iteration\n",
    "# We should define a `step_fn` function\n",
    "# The code below repeats a `keker.default_step_fn` code to provide you with a concept of step function\n",
    "\n",
    "def step_fn(model: torch.nn.Module,\n",
    "            batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Determine what your model will do with your data.\n",
    "\n",
    "    Args:\n",
    "        model: the pytorch module to pass input in\n",
    "        batch: the batch of data from the DataLoader\n",
    "\n",
    "    Returns:\n",
    "        The models forward pass results\n",
    "    \"\"\"\n",
    "    \n",
    "    # you could define here whatever logic you want\n",
    "    inp = batch[\"image\"]  # here we get an \"image\" from our dataset\n",
    "    return model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous preparations was mostly out of scope of Kekas library (except DataKeks creation)\n",
    "# Now let's dive into kekas a little bit\n",
    "\n",
    "# firstly, we create a Keker - the core Kekas class, that provides all the keks for your pipeline\n",
    "keker = Keker(model=model,\n",
    "              dataowner=dataowner,\n",
    "              criterion=criterion,\n",
    "              step_fn=step_fn,                    # previosly defined step function\n",
    "              target_key=\"label\",                 # remember, we defined it in the reader_fn for DataKek?\n",
    "              metrics={\"acc\": accuracy},          # optional, you can not specify any metrics at all\n",
    "              opt=torch.optim.Adam,               # optimizer class. if note specifiyng, \n",
    "                                                  # an SGD is using by default\n",
    "              opt_params={\"weight_decay\": 1e-5})  # optimizer kwargs in dict format (optional too)\n",
    "\n",
    "# Actually, there are a lot of params for kekers, but this out of scope of this example\n",
    "# you can read about them in Keker's docstring (but who really reads the docs, huh?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before the start of the finetuning procedure let's freeeze all the layers except the last one - the head\n",
    "# the `freeze` method is mostly inspired (or stolen) from fastai\n",
    "# but you should define a model's attribute to deal with\n",
    "# for example, our model is actually model.net, so we need to specify the 'net' attr\n",
    "# also this method does not freezes batchnorm layers by default. To change this set `freeze_bn=True`\n",
    "keker.freeze(model_attr=\"net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's find an 'optimal' learning rate with learning rate find procedure\n",
    "# for details please see the fastai course and this articles:\n",
    "# https://arxiv.org/abs/1803.09820\n",
    "# https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n",
    "\n",
    "# NOTE: this is an optional step and you can skip it and use your favorite learning rate\n",
    "\n",
    "# you MUST specify the logdir to see graphics\n",
    "# keker will write a tensorboard logs into this folder\n",
    "# to see them start a tensorboard with `--logdir /path/to/logdir`\n",
    "# OR you can use keker.plot_kek_lr method (see cell below)\n",
    "keker.kek_lr(final_lr=0.1, logdir=\"/path/to/logdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Learning Rate find results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in plot to see on which step the loss was still decreasing\n",
    "# and choose LR from this step\n",
    "keker.plot_kek_lr(logdir=\"/path/to/logdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Kek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now let's start training!\n",
    "# It's as simple as:\n",
    "keker.kek(lr=1e-5, epochs=3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kek with different optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeKekasUser: Wait, and what if I want to train with the different optimizer?\n",
    "#\n",
    "# Me:\n",
    "keker.kek(lr=1e-5, \n",
    "          epochs=1,\n",
    "          opt=torch.optim.RMSprop,            # optimizer class\n",
    "          opt_params={\"weight_decay\": 1e-5})  # optimizer kwargs in dict format (if you want)\n",
    "\n",
    "# by default, the optimizer specified on Keker initialization is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kek with scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeKekasUser: OK, and what if I want to use a pytorch scheduler?\n",
    "#\n",
    "# Me:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=2,\n",
    "          sched=torch.optim.lr_scheduler.StepLR,       # pytorch lr scheduler class\n",
    "          sched_params={\"step_size\":1, \"gamma\": 0.9})  # schedulres kwargas in dict format\n",
    "\n",
    "# by default, no scheduler is using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log your keks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeKekasUser: How about logging?\n",
    "#\n",
    "# Me:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=1,\n",
    "          logdir=\"/mnt/hdd3_4/belskikh/keks/forplot\")\n",
    "\n",
    "# It will create a `train` and `val` subfolders in logdir, and will write tensorboard logs into them\n",
    "# to see them start a tensorboard with `--logdir /path/to/logdir`\n",
    "# OR you can use keker.plot_kek method! (see cell below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot your keks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kekas uses plotly lib and tensorboard logs to plot inside NB\n",
    "keker.plot_kek(logdir=\"/path/to/logdir\",  # path to logdir with logs to plot\n",
    "               step=\"batch\",              # (optional) default is \"step\". another option is \"epoch\"\n",
    "                                          # It determines discreteness of ploting\n",
    "               metrics=[\"loss\",           # (optional) list of metrics names\n",
    "                        \"acc\",            # by default [\"loss\", \"lr\"] is using\n",
    "                        \"lr\"],            # the order of the names determines the order of the plot\n",
    "                                          # NOTE: names of metrics must match names in metrics dict\n",
    "                                          # which was specified on Keker init step\n",
    "               height=1200,               # (optional) height of the total plot \n",
    "               width=800)                 # (optional) width of the total plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeKekasUser: Also I want to save best checkpoints to later use them for SWA or ensembling!\n",
    "#                And I want to measure them by custom metric, control their number, specify their name prefix,\n",
    "#                and control what I need - minimize or maximize metric!\n",
    "# Me: Here it is:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=1,\n",
    "          cp_saver_params={\n",
    "              \"savedir\": \"/path/to/save/dir\",  # a directory for checkpoints\n",
    "              \"metric\": \"acc\",  # (optional) from `metrics` dict on Keker init. \n",
    "                                # default is validation loss\n",
    "              \"n_best\": 3,      # (optional) default is 3\n",
    "              \"prefix\": \"kek\",  # (optional) default prefix is `checkpoint`\n",
    "              \"mode\": \"max\"     # (optional) default is 'min'\n",
    "          })   \n",
    "\n",
    "# It will create a `savedir` directory, and will save best checkpoints there\n",
    "# with naming `{prefix}.{epoch_num}.h5`. The best checkpoint will be dublicated with `{prefix}.best.h5` name\n",
    "# look at the report down here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeKekasUser: Allright, and I don't want to train model, if validation loss doesn't improve for several epochs.\n",
    "# \n",
    "# Me: You mean, early stopping? Here:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=1, \n",
    "          early_stop_params={\n",
    "              \"patience\": 3,   # number of bad epochs to wait before stopping\n",
    "              \"metric\": \"acc\", # (optional) metric name from 'metric' dict. default is val loss\n",
    "              \"mode\": \"min\",   # (optional) what you want from you metric, max or min? default is 'min'\n",
    "              \"min_delta\": 0   # (optional) a minimum delta to count an epoch as 'bad'\n",
    "          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeAdvancedKekasUser: I WANT IT ALL!\n",
    "# \n",
    "# Me: Well, okay then...\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=5,\n",
    "          opt=torch.optim.RMSprop,\n",
    "          opt_params={\"weight_decay\": 1e-5},\n",
    "          sched=torch.optim.lr_scheduler.StepLR,\n",
    "          sched_params={\"step_size\":1, \"gamma\": 0.9},\n",
    "          logdir=\"/path/to/logdir\",\n",
    "          cp_saver_params={\n",
    "              \"savedir\": \"/path/to/save/dir\",  \n",
    "              \"metric\": \"acc\",  \n",
    "              \"n_best\": 3,      \n",
    "              \"prefix\": \"kek\",  \n",
    "              \"mode\": \"max\"},     \n",
    "          early_stop_params={\n",
    "              \"patience\": 3,   \n",
    "              \"metric\": \"acc\", \n",
    "              \"mode\": \"min\",   \n",
    "              \"min_delta\": 0\n",
    "          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Cycle Kek!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeFastaiFan: Did you stole something else from fastai?\n",
    "#\n",
    "# Me: Yes! One Cycle Policy!\n",
    "keker.kek_one_cycle(max_lr=1e-5,                  # the maximum learning rate\n",
    "                    cycle_len=5,                  # number of epochs, actually, but not exactly\n",
    "                    momentum_range=(0.95, 0.85),  # range of momentum changes\n",
    "                    div_factor=25,                # max_lr / min_lr\n",
    "                    increase_fraction=0.3)        # the part of cycle when learning rate increases\n",
    "\n",
    "# If you don't understand these parameters, read this - https://sgugger.github.io/the-1cycle-policy.html\n",
    "# NOTE: you cannot use schedulers and early stopping with one cycle!\n",
    "# another options are the same as for `kek` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Keker features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing / unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've already talk about freezing. But what if I want to unfreeze?\n",
    "# It has the same interface:\n",
    "keker.unfreeze(model_attr=\"net\")\n",
    "\n",
    "# If you want to freeze till some layer:\n",
    "layer_num = -2\n",
    "keker.freeze_to(layer_num, model_attr=\"net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "keker.save(\"/path/to/file\")\n",
    "\n",
    "# loading\n",
    "keker.load(\"/path/to/file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device and DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keker is using all avialable GPUs by default\n",
    "# To limit it, use 'CUDA_VISIBLE_DEVICES' environment variable (available in os.environ dict)\n",
    "\n",
    "# if you want to specify cuda device for your model, specify `device` parameter on Keker initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 4 (yes, four) ways to get a predictions with keker\n",
    "\n",
    "# 1st\n",
    "keker.predict(savepath=\"/path/to/save/dir\")\n",
    "# it will makes predicts on your 'test_dl' dataloader (remember, we initialized it with 'None'), if it specified,\n",
    "# and saves models output in numpy.ndarray format to 'savepath'\n",
    "\n",
    "# 2nd\n",
    "loader = val_dl\n",
    "keker.predict_loader(loader=loader, savepath=\"/path/to/save/dir\")\n",
    "# it will do the same as `predict()` but on any custom loader you want\n",
    "\n",
    "# 3rd\n",
    "tensor = torch.zeros(4, 224, 224, 3)\n",
    "preds = keker.predict_tensor(tensor=tensor, to_numpy=False)\n",
    "# it will return a predictions of the model in numpy format if `'to_numpy==True', else - torch.Tensor\n",
    "\n",
    "# 4th\n",
    "array = np.zeros((4, 224, 224, 3))\n",
    "preds = keker.predict_array(array=array, to_numpy=False)\n",
    "# it will do the same as `predict_tensor()` but with np.ndarra as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am sure that it is not very convinient way for test time augmentations,\n",
    "# but here is how you can do it with Kekas\n",
    "\n",
    "# first, specify several augmentations for TTA\n",
    "flip_ = Flip(always_apply=True)\n",
    "vertical_flip_ = VerticalFlip(always_apply=True)\n",
    "transpose_ = Transpose(always_apply=True)\n",
    "\n",
    "# second, create the whole augmentations with theese ones inside\n",
    "def insert_aug(aug, dataset_key=\"image\", size=224):    \n",
    "    PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n",
    "    \n",
    "    AUGS = Transformer(dataset_key, lambda x: aug(image=x)[\"image\"])\n",
    "    \n",
    "    NRM_TFMS = transforms.Compose([\n",
    "        Transformer(dataset_key, to_torch()),\n",
    "        Transformer(dataset_key, normalize())\n",
    "    ])\n",
    "    \n",
    "    tfm = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n",
    "    return tfm\n",
    "\n",
    "\n",
    "flip = insert_aug(flip_)\n",
    "vertical_flip = insert_aug(vertical_flip_)\n",
    "transpose = insert_aug(transpose_)\n",
    "\n",
    "tta_tfms = {\"flip\": flip, \"v_flip\": vertical_flip, \"transpose\": transpose}\n",
    "\n",
    "# third, run TTA\n",
    "keker.TTA(loader=val_dl,                # loader to predict on \n",
    "          tfms=tta_tfms,                # list or dict of always applying transforms\n",
    "          savedir=\"/path/to/save/dir\",  # savedir\n",
    "          prefix=\"preds\")               # (optional) name prefix. default is 'preds'\n",
    "\n",
    "# it will saves predicts for each augmentation to savedir with name\n",
    "#  - {prefix}_{name_from_dict}.npy if tfms is a dict\n",
    "#  - {prefix}_{index}.npy          if tfms is a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks is the way in which Kekas customizes its pipeline\n",
    "# each callback implements six methods, which names tell when it applies\n",
    "# on_train_begin()\n",
    "#     on_epoch_begin()\n",
    "#         on_batch_begin()\n",
    "#             >>>... step here ...<<<\n",
    "#         on_batch_end()\n",
    "#     on_epoch_end()\n",
    "# on_train_end()\n",
    "\n",
    "# Callbacks are widely using under the hood of Kekas\n",
    "# For example - loss, opimizer, progressbar, lr scheduling, checkpoint saving, early stopping etc\n",
    "# are realized as callbacks\n",
    "\n",
    "# Callback has access to `state` attr of a keker. Here is a docs from Keker about state:\n",
    "\n",
    "        # The state is an object that stores many variables and represents\n",
    "        # the state of your train-val-repdict pipeline. _state passed to every\n",
    "        # callback call.\n",
    "        # You can use it as a container for your custom variables, but\n",
    "        # DO NOT USE the following ones:\n",
    "        #\n",
    "        # loss, batch, model, dataowner, criterion, opt, parallel, checkpoint,\n",
    "        # stop_iter, stop_epoch, stop_train, out, sched, mode, loader, pbar,\n",
    "        # metrics, epoch_metrics\n",
    "\n",
    "# You can write your own callback, or use something useful from kekas.callbacks\n",
    "\n",
    "# Callbacks should be passes as a list at the Keker initiation\n",
    "# For example, let's use a DebuggerCallback, that just insert a pdb.set_trace() call in pipeline\n",
    "# For more info, please see a DebuggerCallback docs and source code\n",
    "debugger = DebuggerCallback(when=[\"on_epoch_begin\"], modes[\"train\"])\n",
    "\n",
    "keker = Keker(model=model, dataowner=dataowner, criterion=criterion, callbacks=[debugger])\n",
    "\n",
    "# also there is a method to add a callbacks to existing Keker\n",
    "\n",
    "keker.add_callbacks([debugger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss and opimizer callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As was said, loss and optimezer behavior is realiesed as Callbacks.\n",
    "# If you use some tricky loss or optimizer logic, you can create your own Callback\n",
    "# and specify it during Keker initialization\n",
    "\n",
    "# here are the callbacks, that are using by default\n",
    "class LossCallback(Callback):\n",
    "    def __init__(self, target_key: str, preds_key: str) -> None:\n",
    "        # target_key and preds_key are the parameters of Keker\n",
    "        self.target_key = target_key\n",
    "        self.preds_key = preds_key\n",
    "\n",
    "    def on_batch_end(self, i: int, state: DotDict) -> None:\n",
    "        target = state.batch[self.target_key]\n",
    "        preds = state.out[self.preds_key]\n",
    "\n",
    "        state.loss = state.criterion(preds, target)\n",
    "\n",
    "class OptimizerCallback(Callback):\n",
    "    def on_batch_end(self, i: int, state: DotDict) -> None:\n",
    "        if state.mode == \"train\":\n",
    "            state.opt.zero_grad()\n",
    "            state.loss.backward()\n",
    "            state.opt.step()\n",
    "            \n",
    "# and here is how you should specify them during Keker initialization\n",
    "keker = Keker(model=model, \n",
    "              dataowner=dataowner,\n",
    "              criterion=criterion,\n",
    "              loss_cb=LossCallback,\n",
    "              opt_cb=OptimizerCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you now got an idea how to use Kekas.\n",
    "\n",
    "I will be happy to get feedback about my library and this tutorial.\n",
    "\n",
    "You can find me in [OpenDataScience](http://ods.ai) community by @belskikh nikname or create an issue on GitHub.\n",
    "\n",
    "Have a good keks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
