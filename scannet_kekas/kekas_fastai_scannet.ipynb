{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roma/.virtualenvs/aseg_torch1/lib/python3.6/site-packages/kekas/keker.py:9: UserWarning: Error 'No module named 'apex''' during importing apex library. To use mixed precison you should install it from https://github.com/NVIDIA/apex\n",
      "  warnings.warn(f\"Error '{e}'' during importing apex library. To use mixed precison\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "#import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from pdb import set_trace as st\n",
    "\n",
    "#import pretrainedmodels as pm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "#from albumentations import Compose, JpegCompression, CLAHE, RandomRotate90, Transpose, ShiftScaleRotate, \\\n",
    "#        Blur, OpticalDistortion, GridDistortion, HueSaturationValue, Flip, VerticalFlip\n",
    "\n",
    "from kekas import Keker, DataOwner, DataKek\n",
    "from kekas.transformations import Transformer, to_torch, normalize\n",
    "from kekas.metrics import accuracy\n",
    "from kekas.modules import Flatten, AdaptiveConcatPool2d\n",
    "from kekas.callbacks import Callback, Callbacks, DebuggerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai_sparse # 3D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparseconvnet as scn\n",
    "\n",
    "from fastai_sparse import utils, visualize\n",
    "from fastai_sparse.utils import log\n",
    "#from fastai_sparse.data import DataSourceConfig, MeshesDataset, SparseDataBunch\n",
    "#from fastai_sparse.learner import SparseModelConfig, Learner\n",
    "#from fastai_sparse.callbacks import TimeLogger, SaveModelCallback, CSVLogger\n",
    "from fastai_sparse.transforms import Transform, Compose\n",
    "\n",
    "from data import merge_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment environment and system metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neptune_callbacks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-31ec532af154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneptune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneptune_callbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeptuneMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neptune_callbacks'"
     ]
    }
   ],
   "source": [
    "import neptune\n",
    "from neptune_callbacks import NeptuneMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_epoch': 384, 'max_lr': 0.5, 'wd': 3e-06}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params={'n_epoch': 256+128,\n",
    "        'max_lr': 0.5,\n",
    "        'wd':0.000003\n",
    "        }\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "with open('NEPTUNE_API_TOKEN.txt','r') as f:\n",
    "    NEPTUNE_API_TOKEN = f.readline().splitlines()[0]\n",
    "    \n",
    "neptune.init(api_token=NEPTUNE_API_TOKEN,\n",
    "             project_qualified_name='roma-goodok/fastai-sparse-scannet')\n",
    "\n",
    "\n",
    "\n",
    "# create experiment in the project defined above\n",
    "exp = neptune.create_experiment(params=params)\n",
    "print(exp.id)\n",
    "exp.append_tag('study')\n",
    "exp.append_tag('kekas')\n",
    "exp.append_tag('unet24')\n",
    "exp.append_tag('1cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kekas'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    experiment_name = exp.id\n",
    "except Exception as e:\n",
    "    experiment_name = \"kekas\"\n",
    "experiment_name        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virtualenv:     (aseg_torch1) \n",
      "python:         3.6.8\n",
      "nvidia driver:  b'384.130'\n",
      "nvidia cuda:    9.0, V9.0.176\n",
      "cudnn:          7.1.4\n",
      "torch:          1.0.0\n",
      "pandas:         0.24.2\n",
      "kekas:          0.1.17\n",
      "fastai:         1.0.48\n",
      "fastai_sparse:  0.0.4.dev0\n"
     ]
    }
   ],
   "source": [
    "utils.watermark(pandas=True, kekas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m69d33ec\u001b[m lr_find\r\n",
      "\u001b[33ma24855f\u001b[m Updates\r\n",
      "\u001b[33m87b37dc\u001b[m Initial commit\r\n"
     ]
    }
   ],
   "source": [
    "!git log1 -n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 23:50:14 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 384.130                Driver Version: 384.130                   |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:28:00.0  On |                  N/A |\r\n",
      "| 29%   57C    P0    69W / 250W |    902MiB / 11163MiB |      2%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:29:00.0 Off |                  N/A |\r\n",
      "|  0%   52C    P2    54W / 250W |     11MiB / 11172MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0       660      G   /usr/lib/firefox/firefox                     186MiB |\r\n",
      "|    0      1442      G   /usr/lib/xorg/Xorg                           364MiB |\r\n",
      "|    0      2538      G   compiz                                       346MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:            AMD Ryzen 7 1700 Eight-Core Processor\r\n"
     ]
    }
   ],
   "source": [
    "!lscpu | grep \"Model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.wide_notebook()\n",
    "# uncomment this lines if you want switch off interactive and save visaulisation as screenshoots:\n",
    "# For rendering run command in terminal:    `chromium-browser --remote-debugging-port=9222`\n",
    "if  False:\n",
    "    visualize.options.interactive = False\n",
    "    visualize.options.save_images = True\n",
    "    visualize.options.verbose = True\n",
    "    visualize.options.filename_pattern_image = Path('images', experiment_name, 'fig_{fig_number}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see how download and preprocess data by the following link from fastai_sparse library: https://github.com/goodok/fastai_sparse/tree/master/examples/scannet/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scene0000_01.merged.ply']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOURCE_DIR = Path('data', 'scannet_merged_ply')\n",
    "assert SOURCE_DIR.exists(), \"Run prepare_data.ipynb\"\n",
    "\n",
    "definition_of_spliting_dir = Path('data', 'ScanNet_Tasks_Benchmark')\n",
    "assert definition_of_spliting_dir.exists()\n",
    "\n",
    "\n",
    "os.listdir(SOURCE_DIR / 'scene0000_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(path, ext='merged.ply'):\n",
    "    pattern = str(path / '*' / ('*' + ext))\n",
    "    fnames = glob.glob(pattern)\n",
    "    return fnames\n",
    "\n",
    "def get_df_list(verbose=0):\n",
    "    # train /valid / test splits\n",
    "    fn_lists = {}\n",
    "\n",
    "    fn_lists['train'] = definition_of_spliting_dir / 'scannetv1_train.txt'\n",
    "    fn_lists['valid'] = definition_of_spliting_dir / 'scannetv1_val.txt'\n",
    "    fn_lists['test'] = definition_of_spliting_dir / 'scannetv1_test.txt'\n",
    "\n",
    "    for datatype in ['train', 'valid', 'test']:\n",
    "        assert fn_lists[datatype].exists(), datatype\n",
    "\n",
    "    dfs = {}\n",
    "    total = 0\n",
    "    for datatype in ['train', 'valid', 'test']:\n",
    "        df = pd.read_csv(fn_lists[datatype], header=None, names=['example_id'])\n",
    "        df = df.assign(datatype=datatype)\n",
    "        df = df.assign(subdir=df.example_id)\n",
    "        df = df.sort_values('example_id')\n",
    "        dfs[datatype] = df\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{datatype:5} counts: {len(df):>4}\")\n",
    "        \n",
    "        total += len(df)\n",
    "    if verbose:\n",
    "        print(f\"total:     {total}\")\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train counts: 1045\n",
      "valid counts:  156\n",
      "test  counts:  312\n",
      "total:     1513\n"
     ]
    }
   ],
   "source": [
    "df_list = get_df_list(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>datatype</th>\n",
       "      <th>subdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>scene0000_00</td>\n",
       "      <td>train</td>\n",
       "      <td>scene0000_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>scene0000_01</td>\n",
       "      <td>train</td>\n",
       "      <td>scene0000_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>scene0000_02</td>\n",
       "      <td>train</td>\n",
       "      <td>scene0000_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>scene0001_00</td>\n",
       "      <td>train</td>\n",
       "      <td>scene0001_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>scene0001_01</td>\n",
       "      <td>train</td>\n",
       "      <td>scene0001_01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       example_id datatype        subdir\n",
       "827  scene0000_00    train  scene0000_00\n",
       "828  scene0000_01    train  scene0000_01\n",
       "829  scene0000_02    train  scene0000_02\n",
       "496  scene0001_00    train  scene0001_00\n",
       "497  scene0001_01    train  scene0001_01"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list['train'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scene0000_00.merged.ply']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(SOURCE_DIR, 'scene0000_00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_sparse.data_items  import MeshItem, PointsItem\n",
    "from fastai_sparse.learner import SparseModelConfig\n",
    "import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at first we need to create a reader function that will define how image will be opened\n",
    "def reader_fn(i, row):\n",
    "    fn = SOURCE_DIR / row['subdir'] / f'{row[\"example_id\"]}.merged.ply'\n",
    "    m = MeshItem.from_file(fn, label_field='label')\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeshItem (scene0000_00.merged.ply)\n",
      "vertices:                shape: (81369, 3)            dtype: float64        min:   -0.01657,  max:    8.74040,  mean:    3.19051\n",
      "faces:                   shape: (153587, 3)           dtype: int64          min:          0,  max:      81368,  mean: 40549.68796\n",
      "colors:                  shape: (81369, 4)            dtype: uint8          min:    1.00000,  max:  255.00000,  mean:  145.80430\n",
      "labels:                  shape: (81369,)              dtype: uint16         min:    0.00000,  max:  230.00000,  mean:   12.97057\n",
      "Colors from vertices\n",
      "Labels from vertices\n"
     ]
    }
   ],
   "source": [
    "m = reader_fn(0, df_list['train'].iloc[0])\n",
    "m.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.is_colors_from_vertices, m.is_labels_from_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map relevant classes to {0,1,...,19}, and ignored classes to -100\n",
    "remapper = np.ones(3000, dtype=np.int32) * (-100)\n",
    "for i, x in enumerate([1,2,3,4,5,6,7,8,9,10,11,12,14,16,24,28,33,34,36,39]):\n",
    "    remapper[x] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TFMS = [T.to_points_cloud(method='vertices', normals=False), \n",
    "            T.remap_labels(remapper=remapper, inplace=False),\n",
    "            T.colors_normalize(),\n",
    "            T.normalize_spatial(),\n",
    "           ]\n",
    "\n",
    "_scale = 20\n",
    "\n",
    "AUGS_TRAIN = [\n",
    "    T.noise_affine(amplitude=0.1),\n",
    "    T.flip_x(p=0.5),\n",
    "    T.scale(scale=_scale),\n",
    "    T.rotate_XY(),\n",
    "    \n",
    "    T.elastic(gran=6 * _scale // 50, mag=40 * _scale / 50),\n",
    "    T.elastic(gran=20 * _scale // 50, mag=160 * _scale / 50),\n",
    "    \n",
    "    T.specific_translate(full_scale=4096),\n",
    "    T.crop_points(low=0, high=4096),\n",
    "    T.colors_noise(amplitude=0.1),\n",
    "]\n",
    "\n",
    "AUGS_VALID = [\n",
    "    T.noise_affine(amplitude=0.1),\n",
    "    T.flip_x(p=0.5),\n",
    "    T.scale(scale=_scale),\n",
    "    T.rotate_XY(),\n",
    "\n",
    "    T.translate(offset=4096 / 2),\n",
    "    T.rand_translate(offset=(-2, 2, 3)),  # low, high, dimention\n",
    "    \n",
    "    T.specific_translate(full_scale=4096),\n",
    "    T.crop_points(low=0, high=4096),\n",
    "    T.colors_noise(amplitude=0.1),\n",
    "        \n",
    "    ]\n",
    "\n",
    "SPARSE_TFMS = [\n",
    "    T.merge_features(ones=False, colors=True, normals=False),\n",
    "    T.to_sparse_voxels(),\n",
    "]\n",
    "\n",
    "\n",
    "# reimplement to_torch\n",
    "def _to_torch(x):\n",
    "    x.coords \n",
    "    x.features\n",
    "    x.labels\n",
    "    \n",
    "    return x\n",
    "\n",
    "# to_torch = Transform(_to_torch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function SparseDataBunch.merge_fn at 0x7fe2bbc081e0>, keys_lists=['id', 'labels_raw', 'filtred_mask', 'random_seed', 'num_points'], separate_labels=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import merge_fn\n",
    "merge_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(dataset_key):\n",
    "        \n",
    "    return  Compose(PRE_TFMS + AUGS_TRAIN + SPARSE_TFMS), Compose(PRE_TFMS + AUGS_VALID + SPARSE_TFMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataKeks creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_list['train']\n",
    "val_df = df_list['valid']\n",
    "\n",
    "# now let's create DataKeks\n",
    "train_tfms, val_tfms = get_transforms(\"mesh\")\n",
    "\n",
    "train_dk = DataKek(df=train_df, reader_fn=reader_fn, transforms=train_tfms)\n",
    "val_dk = DataKek(df=val_df, reader_fn=reader_fn, transforms=val_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: scene0000_00.merged\n",
      "coords                   shape: (81369, 3)            dtype: int64          min:         37,  max:       2087,  mean: 1058.10291\n",
      "features                 shape: (81369, 3)            dtype: float32        min:   -0.98522,  max:    1.26155,  mean:   -0.05550\n",
      "x                        shape: (81369,)              dtype: int64          min:         37,  max:        225,  mean:  122.44975\n",
      "y                        shape: (81369,)              dtype: int64          min:       1949,  max:       2087,  mean: 2016.65787\n",
      "z                        shape: (81369,)              dtype: int64          min:       1001,  max:       1089,  mean: 1035.20110\n",
      "labels                   shape: (81369,)              dtype: int64          min:       -100,  max:         17,  mean:  -48.51506\n",
      "voxels: 49117\n",
      "points / voxels: 1.656636195207362\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40937dede53249f9aee6e2842114ff07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = train_dk[0]\n",
    "b.describe()\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and DataLoaders\n",
    "#batch_size = 32\n",
    "#workers = \n",
    "\n",
    "train_dl = DataLoader(train_dk, batch_size=32, num_workers=8, shuffle=True, drop_last=True, collate_fn=merge_fn, pin_memory=False)\n",
    "val_dl = DataLoader(val_dk, batch_size=2, num_workers=2, shuffle=False, collate_fn=merge_fn, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl.pin_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dl.pin_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_dl):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coords': tensor([[3103, 1525, 1590,    0],\n",
       "         [3103, 1525, 1591,    0],\n",
       "         [3103, 1526, 1591,    0],\n",
       "         ...,\n",
       "         [  44, 3057, 3422,   31],\n",
       "         [  44, 3057, 3422,   31],\n",
       "         [  44, 3057, 3422,   31]]),\n",
       " 'features': tensor([[-0.0393, -0.1166, -0.2235],\n",
       "         [-0.0549, -0.1323, -0.2549],\n",
       "         [-0.0706, -0.1479, -0.3020],\n",
       "         ...,\n",
       "         [-0.4921, -0.5751, -0.4440],\n",
       "         [-0.4999, -0.5751, -0.4597],\n",
       "         [-0.2411, -0.3242, -0.1930]]),\n",
       " 'labels': tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       " 'id': ['scene0470_00.merged',\n",
       "  'scene0662_00.merged',\n",
       "  'scene0235_00.merged',\n",
       "  'scene0654_01.merged',\n",
       "  'scene0576_02.merged',\n",
       "  'scene0368_00.merged',\n",
       "  'scene0360_00.merged',\n",
       "  'scene0099_01.merged',\n",
       "  'scene0254_00.merged',\n",
       "  'scene0031_01.merged',\n",
       "  'scene0511_01.merged',\n",
       "  'scene0150_01.merged',\n",
       "  'scene0121_02.merged',\n",
       "  'scene0501_01.merged',\n",
       "  'scene0128_00.merged',\n",
       "  'scene0369_02.merged',\n",
       "  'scene0649_00.merged',\n",
       "  'scene0569_00.merged',\n",
       "  'scene0449_02.merged',\n",
       "  'scene0673_00.merged',\n",
       "  'scene0380_01.merged',\n",
       "  'scene0302_00.merged',\n",
       "  'scene0199_00.merged',\n",
       "  'scene0601_00.merged',\n",
       "  'scene0399_01.merged',\n",
       "  'scene0444_00.merged',\n",
       "  'scene0029_01.merged',\n",
       "  'scene0348_01.merged',\n",
       "  'scene0563_00.merged',\n",
       "  'scene0166_00.merged',\n",
       "  'scene0210_00.merged',\n",
       "  'scene0582_01.merged'],\n",
       " 'labels_raw': [array([0, 0, 0, 0, ..., 0, 4, 4, 4], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ...,    0,    0,    0, -100], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ..., -100, -100, -100, -100], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ..., -100, -100, -100, -100], dtype=int32),\n",
       "  array([  17, -100, -100, -100, ..., -100, -100, -100, -100], dtype=int32),\n",
       "  array([1, 1, 1, 1, ..., 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 2, 0, ..., 0, 0, 0, 0], dtype=int32),\n",
       "  array([   0,    0,    0,    0, ..., -100, -100, -100, -100], dtype=int32),\n",
       "  array([0, 0, 0, 0, ..., 0, 0, 0, 0], dtype=int32),\n",
       "  array([   2,    2,    2,    2, ..., -100, -100, -100, -100], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ..., -100, -100, -100, -100], dtype=int32),\n",
       "  array([-100, -100,    2,    2, ...,    0,    0,    0,    0], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ..., -100, -100, -100, -100], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ..., -100, -100, -100, -100], dtype=int32),\n",
       "  array([   2,    2,    2,    2, ..., -100, -100, -100, -100], dtype=int32),\n",
       "  array([2, 2, 2, 2, ..., 0, 0, 0, 0], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ...,    0,    0,    0, -100], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ...,    0,    0,    0,    0], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ..., -100, -100, -100, -100], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ..., -100,    0,    0,    0], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ..., -100, -100, -100, -100], dtype=int32),\n",
       "  array([2, 2, 2, 2, ..., 0, 0, 0, 0], dtype=int32),\n",
       "  array([2, 2, 2, 2, ..., 0, 0, 0, 0], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ...,    0,    0,    0,    0], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ...,    2,    2,    2,    2], dtype=int32),\n",
       "  array([6, 6, 6, 6, ..., 0, 0, 0, 0], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ..., -100,    0,    0,    0], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ...,    0,    0,    0,    0], dtype=int32),\n",
       "  array([-100, -100, -100, -100, ..., -100, -100, -100, -100], dtype=int32),\n",
       "  array([   2, -100, -100, -100, ..., -100, -100,    0,    0], dtype=int32),\n",
       "  array([0, 0, 6, 6, ..., 2, 2, 2, 2], dtype=int32),\n",
       "  array([7, 7, 7, 7, ..., 0, 0, 0, 0], dtype=int32)],\n",
       " 'filtred_mask': [array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True]),\n",
       "  array([ True,  True,  True,  True, ...,  True,  True,  True,  True])],\n",
       " 'random_seed': ['182479695_340',\n",
       "  '1292740222_264',\n",
       "  '1741434149_312',\n",
       "  '2823988168_140',\n",
       "  '649861767_228',\n",
       "  '2941211774_112',\n",
       "  '3379533227_440',\n",
       "  '296525474_80',\n",
       "  '207020725_440',\n",
       "  '497598068_40',\n",
       "  '3168221195_300',\n",
       "  '2881255566_336',\n",
       "  '806421572_224',\n",
       "  '1119037316_116',\n",
       "  '2052385399_276',\n",
       "  '1697349261_388',\n",
       "  '3481073715_56',\n",
       "  '225097249_100',\n",
       "  '157631320_492',\n",
       "  '100342_396',\n",
       "  '1848795919_364',\n",
       "  '2102817586_576',\n",
       "  '2361204992_576',\n",
       "  '3008858478_444',\n",
       "  '2221092942_92',\n",
       "  '3021209329_532',\n",
       "  '1617108327_108',\n",
       "  '3246590741_108',\n",
       "  '1593665656_304',\n",
       "  '3184915984_348',\n",
       "  '598897525_52',\n",
       "  '940621256_364'],\n",
       " 'num_points': [67465,\n",
       "  46821,\n",
       "  191926,\n",
       "  187822,\n",
       "  130258,\n",
       "  100591,\n",
       "  81298,\n",
       "  170918,\n",
       "  119777,\n",
       "  235122,\n",
       "  83484,\n",
       "  77935,\n",
       "  98699,\n",
       "  111898,\n",
       "  108008,\n",
       "  153044,\n",
       "  70554,\n",
       "  142057,\n",
       "  59822,\n",
       "  254321,\n",
       "  164540,\n",
       "  292241,\n",
       "  170183,\n",
       "  209655,\n",
       "  84554,\n",
       "  25481,\n",
       "  41212,\n",
       "  71272,\n",
       "  129766,\n",
       "  225624,\n",
       "  114103,\n",
       "  166400]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4186851, 4186851)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch['coords']), sum(batch['num_points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseModelConfig;\n",
       "   spatial_size: 4096\n",
       "   dimension: 3\n",
       "   block_reps: 1\n",
       "   m: 16\n",
       "   num_planes: [16, 32, 48, 64, 80, 96, 112]\n",
       "   residual_blocks: False\n",
       "   num_classes: 20\n",
       "   num_input_features: 3\n",
       "   mode: 4\n",
       "   downsample: [2, 2]\n",
       "   bias: False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spatial_size  is full_scale\n",
    "model_config = SparseModelConfig(spatial_size=4096, num_classes=20, num_input_features=3, mode=4,\n",
    "                                 m=16, num_planes_coeffs=[1, 2, 3, 4, 5, 6, 7])\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        nn.Module.__init__(self)\n",
    "        self.sparseModel = scn.Sequential(\n",
    "            scn.InputLayer(cfg.dimension, cfg.spatial_size, mode=cfg.mode),\n",
    "            scn.SubmanifoldConvolution(cfg.dimension, nIn=cfg.num_input_features, nOut=cfg.m, filter_size=3, bias=cfg.bias),\n",
    "            scn.UNet(cfg.dimension, cfg.block_reps, cfg.num_planes, residual_blocks=cfg.residual_blocks, downsample=cfg.downsample),\n",
    "            scn.BatchNormReLU(cfg.m),\n",
    "            scn.OutputLayer(cfg.dimension),\n",
    "        )\n",
    "        self.linear = nn.Linear(cfg.m, cfg.num_classes)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        x = [xb['coords'], xb['features']]\n",
    "        x = self.sparseModel(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "model = Model(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the three whales of your pipelane are: the data, the model and the loss (hi, Jeremy)\n",
    "\n",
    "# the data is represented in Kekas by DataOwner. It is a namedtuple with three fields:\n",
    "# 'train_dl', 'val_dl', 'test_dl'\n",
    "# For training process we will need at least two of them, and we can skip 'test_dl' for now\n",
    "# so we will initialize it with `None` value.\n",
    "dataowner = DataOwner(train_dl, val_dl, None)\n",
    "\n",
    "# model is just a pytorch nn.Module, that we created vefore\n",
    "#model = Net(num_classes=2)\n",
    "\n",
    "# loss or criterion is also a pytorch nn.Module. For multiloss scenarios it can be a list of nn.Modules\n",
    "# for our simple example let's use the standart cross entopy criterion\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also we need to specify, what model will do with each batch of data on each iteration\n",
    "# We should define a `step_fn` function\n",
    "# The code below repeats a `keker.default_step_fn` code to provide you with a concept of step function\n",
    "\n",
    "def step_fn(model: torch.nn.Module,\n",
    "            batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Determine what your model will do with your data.\n",
    "\n",
    "    Args:\n",
    "        model: the pytorch module to pass input in\n",
    "        batch: the batch of data from the DataLoader\n",
    "\n",
    "    Returns:\n",
    "        The models forward pass results\n",
    "    \"\"\"\n",
    "    \n",
    "    # you could define here whatever logic you want\n",
    "    inp = batch  # here we get an \"image\" from our dataset\n",
    "    return model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous preparations was mostly out of scope of Kekas library (except DataKeks creation)\n",
    "# Now let's dive into kekas a little bit\n",
    "\n",
    "# firstly, we create a Keker - the core Kekas class, that provides all the keks for your pipeline\n",
    "keker = Keker(model=model,\n",
    "              dataowner=dataowner,\n",
    "              criterion=criterion,\n",
    "              step_fn=step_fn,                    # previosly defined step function\n",
    "              target_key=\"labels\",                 # remember, we defined it in the reader_fn for DataKek?              \n",
    "              opt=torch.optim.Adam,               # optimizer class. if note specifiyng, \n",
    "                                                  # an SGD is using by default\n",
    "              opt_params={\"weight_decay\": 1e-5})  # optimizer kwargs in dict format (optional too)\n",
    "\n",
    "# Actually, there are a lot of params for kekers, but this out of scope of this example\n",
    "# you can read about them in Keker's docstring (but who really reads the docs, huh?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before the start of the finetuning procedure let's freeeze all the layers except the last one - the head\n",
    "# the `freeze` method is mostly inspired (or stolen) from fastai\n",
    "# but you should define a model's attribute to deal with\n",
    "# for example, our model is actually model.net, so we need to specify the 'net' attr\n",
    "# also this method does not freezes batchnorm layers by default. To change this set `freeze_bn=True`\n",
    "\n",
    "#keker.freeze(model_attr=\"net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100% 32/32 [03:09<00:00,  4.23s/it, loss=2.4019]\n",
      "End of LRFinder\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's find an 'optimal' learning rate with learning rate find procedure\n",
    "# for details please see the fastai course and this articles:\n",
    "# https://arxiv.org/abs/1803.09820\n",
    "# https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n",
    "\n",
    "# NOTE: this is an optional step and you can skip it and use your favorite learning rate\n",
    "\n",
    "# you MUST specify the logdir to see graphics\n",
    "# keker will write a tensorboard logs into this folder\n",
    "# to see them start a tensorboard with `--logdir /path/to/logdir`\n",
    "# OR you can use keker.plot_kek_lr method (see cell below)\n",
    "keker.kek_lr(final_lr=0.1, logdir=\"logdir_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "name": "train/batch/loss",
         "type": "scatter",
         "uid": "3cfc8c95-69f0-48f7-9865-3696748d940c",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31
         ],
         "y": [
          3.1508469581604004,
          3.155036687850952,
          3.1586554050445557,
          3.1575560569763184,
          3.15378999710083,
          3.157327651977539,
          3.1372246742248535,
          3.147700309753418,
          3.154764175415039,
          3.154165029525757,
          3.164154291152954,
          3.139570951461792,
          3.1136326789855957,
          3.153083562850952,
          3.1389577388763428,
          3.1245481967926025,
          3.1077356338500977,
          3.121633529663086,
          3.0495405197143555,
          2.961421489715576,
          2.9264042377471924,
          2.8268916606903076,
          2.728623867034912,
          2.633754014968872,
          2.5442726612091064,
          2.4414467811584473,
          2.356302261352539,
          2.1476333141326904,
          1.9068760871887207,
          1.7697784900665283,
          1.7481977939605713,
          1.418802261352539
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "batch/loss"
        },
        "yaxis": {
         "hoverformat": ".6f"
        }
       }
      },
      "text/html": [
       "<div id=\"a5aa72f6-c5bf-4b32-91db-68b25e6dfb22\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"a5aa72f6-c5bf-4b32-91db-68b25e6dfb22\")) {\n",
       "    Plotly.newPlot(\"a5aa72f6-c5bf-4b32-91db-68b25e6dfb22\", [{\"name\": \"train/batch/loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], \"y\": [3.1508469581604004, 3.155036687850952, 3.1586554050445557, 3.1575560569763184, 3.15378999710083, 3.157327651977539, 3.1372246742248535, 3.147700309753418, 3.154764175415039, 3.154165029525757, 3.164154291152954, 3.139570951461792, 3.1136326789855957, 3.153083562850952, 3.1389577388763428, 3.1245481967926025, 3.1077356338500977, 3.121633529663086, 3.0495405197143555, 2.961421489715576, 2.9264042377471924, 2.8268916606903076, 2.728623867034912, 2.633754014968872, 2.5442726612091064, 2.4414467811584473, 2.356302261352539, 2.1476333141326904, 1.9068760871887207, 1.7697784900665283, 1.7481977939605713, 1.418802261352539], \"type\": \"scatter\", \"uid\": \"3cfc8c95-69f0-48f7-9865-3696748d940c\"}], {\"title\": {\"text\": \"batch/loss\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"a5aa72f6-c5bf-4b32-91db-68b25e6dfb22\")) {window._Plotly.Plots.resize(document.getElementById(\"a5aa72f6-c5bf-4b32-91db-68b25e6dfb22\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"a5aa72f6-c5bf-4b32-91db-68b25e6dfb22\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"a5aa72f6-c5bf-4b32-91db-68b25e6dfb22\")) {\n",
       "    Plotly.newPlot(\"a5aa72f6-c5bf-4b32-91db-68b25e6dfb22\", [{\"name\": \"train/batch/loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], \"y\": [3.1508469581604004, 3.155036687850952, 3.1586554050445557, 3.1575560569763184, 3.15378999710083, 3.157327651977539, 3.1372246742248535, 3.147700309753418, 3.154764175415039, 3.154165029525757, 3.164154291152954, 3.139570951461792, 3.1136326789855957, 3.153083562850952, 3.1389577388763428, 3.1245481967926025, 3.1077356338500977, 3.121633529663086, 3.0495405197143555, 2.961421489715576, 2.9264042377471924, 2.8268916606903076, 2.728623867034912, 2.633754014968872, 2.5442726612091064, 2.4414467811584473, 2.356302261352539, 2.1476333141326904, 1.9068760871887207, 1.7697784900665283, 1.7481977939605713, 1.418802261352539], \"type\": \"scatter\", \"uid\": \"3cfc8c95-69f0-48f7-9865-3696748d940c\"}], {\"title\": {\"text\": \"batch/loss\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"a5aa72f6-c5bf-4b32-91db-68b25e6dfb22\")) {window._Plotly.Plots.resize(document.getElementById(\"a5aa72f6-c5bf-4b32-91db-68b25e6dfb22\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "name": "train/batch/lr",
         "type": "scatter",
         "uid": "a42398fb-047c-41dd-821f-cf43a25d134e",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31
         ],
         "y": [
          9.999999974752427e-07,
          1.4497406937152846e-06,
          2.1017481230956037e-06,
          3.046989604627015e-06,
          4.417344825924374e-06,
          6.404004125215579e-06,
          9.284145562560298e-06,
          1.3459602996590547e-05,
          1.951293415913824e-05,
          2.8288694011280313e-05,
          4.101126978639513e-05,
          5.945570592302829e-05,
          8.619535947218537e-05,
          0.00012496091949287802,
          0.0001811609254218638,
          0.0002626363420858979,
          0.0003807546163443476,
          0.0005519954138435423,
          0.0008002502145245671,
          0.0011601552832871675,
          0.00168192433193326,
          0.0024383540730923414,
          0.0035349810495972633,
          0.005124805960804224,
          0.007429639343172312,
          0.010771050117909908,
          0.015615230426192284,
          0.02263803407549858,
          0.03281927853822708,
          0.04757944494485855,
          0.06897785514593124,
          0.10000000149011612
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "batch/lr"
        },
        "yaxis": {
         "hoverformat": ".6f"
        }
       }
      },
      "text/html": [
       "<div id=\"0cb55059-5750-4cc6-ab3c-42bf86a74671\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"0cb55059-5750-4cc6-ab3c-42bf86a74671\")) {\n",
       "    Plotly.newPlot(\"0cb55059-5750-4cc6-ab3c-42bf86a74671\", [{\"name\": \"train/batch/lr\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], \"y\": [9.999999974752427e-07, 1.4497406937152846e-06, 2.1017481230956037e-06, 3.046989604627015e-06, 4.417344825924374e-06, 6.404004125215579e-06, 9.284145562560298e-06, 1.3459602996590547e-05, 1.951293415913824e-05, 2.8288694011280313e-05, 4.101126978639513e-05, 5.945570592302829e-05, 8.619535947218537e-05, 0.00012496091949287802, 0.0001811609254218638, 0.0002626363420858979, 0.0003807546163443476, 0.0005519954138435423, 0.0008002502145245671, 0.0011601552832871675, 0.00168192433193326, 0.0024383540730923414, 0.0035349810495972633, 0.005124805960804224, 0.007429639343172312, 0.010771050117909908, 0.015615230426192284, 0.02263803407549858, 0.03281927853822708, 0.04757944494485855, 0.06897785514593124, 0.10000000149011612], \"type\": \"scatter\", \"uid\": \"a42398fb-047c-41dd-821f-cf43a25d134e\"}], {\"title\": {\"text\": \"batch/lr\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"0cb55059-5750-4cc6-ab3c-42bf86a74671\")) {window._Plotly.Plots.resize(document.getElementById(\"0cb55059-5750-4cc6-ab3c-42bf86a74671\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"0cb55059-5750-4cc6-ab3c-42bf86a74671\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"0cb55059-5750-4cc6-ab3c-42bf86a74671\")) {\n",
       "    Plotly.newPlot(\"0cb55059-5750-4cc6-ab3c-42bf86a74671\", [{\"name\": \"train/batch/lr\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], \"y\": [9.999999974752427e-07, 1.4497406937152846e-06, 2.1017481230956037e-06, 3.046989604627015e-06, 4.417344825924374e-06, 6.404004125215579e-06, 9.284145562560298e-06, 1.3459602996590547e-05, 1.951293415913824e-05, 2.8288694011280313e-05, 4.101126978639513e-05, 5.945570592302829e-05, 8.619535947218537e-05, 0.00012496091949287802, 0.0001811609254218638, 0.0002626363420858979, 0.0003807546163443476, 0.0005519954138435423, 0.0008002502145245671, 0.0011601552832871675, 0.00168192433193326, 0.0024383540730923414, 0.0035349810495972633, 0.005124805960804224, 0.007429639343172312, 0.010771050117909908, 0.015615230426192284, 0.02263803407549858, 0.03281927853822708, 0.04757944494485855, 0.06897785514593124, 0.10000000149011612], \"type\": \"scatter\", \"uid\": \"a42398fb-047c-41dd-821f-cf43a25d134e\"}], {\"title\": {\"text\": \"batch/lr\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"0cb55059-5750-4cc6-ab3c-42bf86a74671\")) {window._Plotly.Plots.resize(document.getElementById(\"0cb55059-5750-4cc6-ab3c-42bf86a74671\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keker.plot_kek_lr(logdir=\"logdir_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100% 32/32 [03:37<00:00,  4.00s/it, loss=3.1426, val_loss=3.0657]\n",
      "Epoch 2/3: 100% 32/32 [03:32<00:00,  3.92s/it, loss=3.1161, val_loss=3.0653]\n",
      "Epoch 3/3: 100% 32/32 [03:35<00:00,  4.08s/it, loss=3.0928, val_loss=3.0375]\n"
     ]
    }
   ],
   "source": [
    "# Ok, now let's start training!\n",
    "# It's as simple as:\n",
    "keker.kek(lr=1e-5, epochs=3, logdir='logdir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "name": "train/batch/loss",
         "type": "scatter",
         "uid": "6e9aae70-9108-48f8-9079-65ecea75af04",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251
         ],
         "y": [
          3.167451858520508,
          3.162532091140747,
          3.15464448928833,
          3.1606154441833496,
          3.146578311920166,
          3.15742826461792,
          3.1474761962890625,
          3.1663851737976074,
          3.144502878189087,
          3.1639976501464844,
          3.150343894958496,
          3.139404535293579,
          3.1671159267425537,
          3.14121150970459,
          3.124532699584961,
          3.1540186405181885,
          3.1349425315856934,
          3.1556379795074463,
          3.1527435779571533,
          3.13120698928833,
          3.1529042720794678,
          3.1199073791503906,
          3.1332168579101562,
          3.1448779106140137,
          3.141596794128418,
          3.1380515098571777,
          3.1274564266204834,
          3.145880699157715,
          3.101627826690674,
          3.1542067527770996,
          3.1539969444274902,
          3.148508310317993,
          3.1634819507598877,
          3.133809804916382,
          3.138721466064453,
          3.150353193283081,
          3.1173691749572754,
          3.1176722049713135,
          3.136475086212158,
          3.1123249530792236,
          3.1671605110168457,
          3.1263725757598877,
          3.122121810913086,
          3.1426544189453125,
          3.1178863048553467,
          3.127592086791992,
          3.1111955642700195,
          3.1351895332336426,
          3.1124300956726074,
          3.1237497329711914,
          3.125225782394409,
          3.1260149478912354,
          3.1143271923065186,
          3.1206130981445312,
          3.0933518409729004,
          3.1029062271118164,
          3.1574935913085938,
          3.136378526687622,
          3.1053285598754883,
          3.106792449951172,
          3.0892813205718994,
          3.098935842514038,
          3.1085591316223145,
          3.1085927486419678,
          3.1118991374969482,
          3.11784291267395,
          3.0874624252319336,
          3.1045303344726562,
          3.1117937564849854,
          3.1020867824554443,
          3.120518445968628,
          3.1075103282928467,
          3.139374256134033,
          3.0963070392608643,
          3.1089253425598145,
          3.1292924880981445,
          3.0958240032196045,
          3.102844715118408,
          3.095151662826538,
          3.119011878967285,
          3.1006410121917725,
          3.095897674560547,
          3.082656145095825,
          3.0877692699432373,
          3.089799404144287,
          3.091919422149658,
          3.101266384124756,
          3.083491086959839,
          3.083766460418701,
          3.106652021408081,
          3.1088266372680664,
          3.0897436141967773,
          3.0532262325286865,
          3.088233709335327,
          3.088864803314209,
          3.090498685836792
         ]
        },
        {
         "name": "val/batch/loss",
         "type": "scatter",
         "uid": "0caab848-81f0-4c1b-ae8c-d97ef979dae8",
         "x": [
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329
         ],
         "y": [
          3.076899766921997,
          2.9726452827453613,
          3.1033992767333984,
          3.153714418411255,
          3.0845162868499756,
          3.1733736991882324,
          3.0366413593292236,
          2.991420269012451,
          3.029853105545044,
          3.0561575889587402,
          3.089643716812134,
          3.1641247272491455,
          3.069410800933838,
          3.096703290939331,
          3.107998847961426,
          3.005009174346924,
          3.0272531509399414,
          2.911414861679077,
          2.920255422592163,
          3.0566365718841553,
          3.1749534606933594,
          3.0513916015625,
          3.115476608276367,
          3.0840466022491455,
          3.0515360832214355,
          2.987941026687622,
          3.0038414001464844,
          3.038910150527954,
          3.0202829837799072,
          3.016558885574341,
          3.2455899715423584,
          3.1052491664886475,
          3.0572588443756104,
          3.0515105724334717,
          3.0377981662750244,
          3.0452330112457275,
          3.0991578102111816,
          3.0559816360473633,
          3.061357021331787,
          3.059131383895874,
          2.985342502593994,
          3.0904595851898193,
          3.08606219291687,
          3.093388557434082,
          3.007359743118286,
          3.1333413124084473,
          3.106509208679199,
          3.0583105087280273,
          3.1041414737701416,
          3.1115424633026123,
          3.001920223236084,
          3.0732688903808594,
          2.960318088531494,
          2.9173600673675537,
          3.0873351097106934,
          3.0670406818389893,
          3.0872550010681152,
          3.0068886280059814,
          3.0486221313476562,
          3.019700050354004,
          3.0560474395751953,
          3.115384340286255,
          3.035614013671875,
          3.140674114227295,
          3.0731594562530518,
          3.0713987350463867,
          3.139779806137085,
          3.1249561309814453,
          3.139899730682373,
          3.066905975341797,
          3.07547926902771,
          3.0088305473327637,
          3.132736921310425,
          3.084552049636841,
          3.0752618312835693,
          3.0956380367279053,
          3.0409109592437744,
          3.1076512336730957,
          3.073855400085449,
          2.927483081817627,
          3.093118190765381,
          3.1720619201660156,
          3.082768440246582,
          3.3045554161071777,
          3.031001329421997,
          2.981029987335205,
          3.020237684249878,
          3.0408072471618652,
          3.0860419273376465,
          3.203688859939575,
          3.0640110969543457,
          3.0930042266845703,
          3.1142187118530273,
          2.9603514671325684,
          3.0052123069763184,
          2.873762607574463,
          2.878046989440918,
          3.041826009750366,
          3.2118191719055176,
          3.055739402770996,
          3.123413562774658,
          3.076700210571289,
          3.0473227500915527,
          2.9730708599090576,
          2.9944913387298584,
          3.018778085708618,
          3.007549285888672,
          3.0063607692718506,
          3.346477746963501,
          3.1338038444519043,
          3.059032917022705,
          3.0408663749694824,
          3.0222554206848145,
          3.044818162918091,
          3.0835349559783936,
          3.0572776794433594,
          3.07576322555542,
          3.080035448074341,
          2.9513967037200928,
          3.070218324661255,
          3.0603528022766113,
          3.0891590118408203,
          2.998645544052124,
          3.147351026535034,
          3.109659194946289,
          3.0609912872314453,
          3.1192171573638916,
          3.1319079399108887,
          2.978424072265625,
          3.082627534866333,
          2.9208109378814697,
          2.867196559906006,
          3.08172869682312,
          3.055131435394287,
          3.084787368774414,
          2.9841830730438232,
          3.046999454498291,
          2.9930124282836914,
          3.0566089153289795,
          3.1343297958374023,
          3.0260303020477295,
          3.13624906539917,
          3.081401824951172,
          3.0839779376983643,
          3.187434673309326,
          3.1606287956237793,
          3.152599573135376,
          3.0711240768432617,
          3.068906545639038,
          3.0155811309814453,
          3.175739049911499,
          3.086963653564453,
          3.070141553878784,
          3.0942633152008057,
          3.026381492614746,
          3.1261751651763916,
          3.0495266914367676,
          2.8793816566467285,
          3.0667808055877686,
          3.1512882709503174,
          3.0499534606933594,
          3.326303720474243,
          3.005032777786255,
          2.9523653984069824,
          2.9960477352142334,
          3.016653537750244,
          3.058884620666504,
          3.1768507957458496,
          3.0357415676116943,
          3.0659289360046387,
          3.0995562076568604,
          2.9201819896698,
          2.969935894012451,
          2.8359713554382324,
          2.845785140991211,
          3.0155398845672607,
          3.181818723678589,
          3.0284786224365234,
          3.097358465194702,
          3.0441455841064453,
          3.0314817428588867,
          2.940817356109619,
          2.9581944942474365,
          2.9858312606811523,
          2.973806619644165,
          2.979299545288086,
          3.330946207046509,
          3.1058387756347656,
          3.0345535278320312,
          3.0159218311309814,
          2.993760347366333,
          3.0161967277526855,
          3.0519063472747803,
          3.029135227203369,
          3.049832582473755,
          3.0592989921569824,
          2.9126152992248535,
          3.0449182987213135,
          3.0341525077819824,
          3.069214105606079,
          2.96368408203125,
          3.113215446472168,
          3.0816402435302734,
          3.0311684608459473,
          3.0939278602600098,
          3.112666606903076,
          2.9379377365112305,
          3.051417827606201,
          2.8816776275634766,
          2.820495367050171,
          3.050798177719116,
          3.019838333129883,
          3.057678699493408,
          2.9465723037719727,
          3.0256717205047607,
          2.9558475017547607,
          3.03145432472229,
          3.1114063262939453,
          2.9969310760498047,
          3.1079041957855225,
          3.0579919815063477,
          3.0631470680236816,
          3.1793885231018066,
          3.1155495643615723,
          3.110666036605835,
          3.040452718734741,
          3.03928279876709,
          3.003910541534424,
          3.1495163440704346,
          3.0555853843688965,
          3.0443732738494873,
          3.0742266178131104,
          3.007148027420044,
          3.1062674522399902
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "batch/loss"
        },
        "yaxis": {
         "hoverformat": ".6f"
        }
       }
      },
      "text/html": [
       "<div id=\"ec72cba7-a561-4ea7-8049-ee5e616f8f14\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"ec72cba7-a561-4ea7-8049-ee5e616f8f14\")) {\n",
       "    Plotly.newPlot(\"ec72cba7-a561-4ea7-8049-ee5e616f8f14\", [{\"name\": \"train/batch/loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], \"y\": [3.167451858520508, 3.162532091140747, 3.15464448928833, 3.1606154441833496, 3.146578311920166, 3.15742826461792, 3.1474761962890625, 3.1663851737976074, 3.144502878189087, 3.1639976501464844, 3.150343894958496, 3.139404535293579, 3.1671159267425537, 3.14121150970459, 3.124532699584961, 3.1540186405181885, 3.1349425315856934, 3.1556379795074463, 3.1527435779571533, 3.13120698928833, 3.1529042720794678, 3.1199073791503906, 3.1332168579101562, 3.1448779106140137, 3.141596794128418, 3.1380515098571777, 3.1274564266204834, 3.145880699157715, 3.101627826690674, 3.1542067527770996, 3.1539969444274902, 3.148508310317993, 3.1634819507598877, 3.133809804916382, 3.138721466064453, 3.150353193283081, 3.1173691749572754, 3.1176722049713135, 3.136475086212158, 3.1123249530792236, 3.1671605110168457, 3.1263725757598877, 3.122121810913086, 3.1426544189453125, 3.1178863048553467, 3.127592086791992, 3.1111955642700195, 3.1351895332336426, 3.1124300956726074, 3.1237497329711914, 3.125225782394409, 3.1260149478912354, 3.1143271923065186, 3.1206130981445312, 3.0933518409729004, 3.1029062271118164, 3.1574935913085938, 3.136378526687622, 3.1053285598754883, 3.106792449951172, 3.0892813205718994, 3.098935842514038, 3.1085591316223145, 3.1085927486419678, 3.1118991374969482, 3.11784291267395, 3.0874624252319336, 3.1045303344726562, 3.1117937564849854, 3.1020867824554443, 3.120518445968628, 3.1075103282928467, 3.139374256134033, 3.0963070392608643, 3.1089253425598145, 3.1292924880981445, 3.0958240032196045, 3.102844715118408, 3.095151662826538, 3.119011878967285, 3.1006410121917725, 3.095897674560547, 3.082656145095825, 3.0877692699432373, 3.089799404144287, 3.091919422149658, 3.101266384124756, 3.083491086959839, 3.083766460418701, 3.106652021408081, 3.1088266372680664, 3.0897436141967773, 3.0532262325286865, 3.088233709335327, 3.088864803314209, 3.090498685836792], \"type\": \"scatter\", \"uid\": \"6e9aae70-9108-48f8-9079-65ecea75af04\"}, {\"name\": \"val/batch/loss\", \"x\": [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329], \"y\": [3.076899766921997, 2.9726452827453613, 3.1033992767333984, 3.153714418411255, 3.0845162868499756, 3.1733736991882324, 3.0366413593292236, 2.991420269012451, 3.029853105545044, 3.0561575889587402, 3.089643716812134, 3.1641247272491455, 3.069410800933838, 3.096703290939331, 3.107998847961426, 3.005009174346924, 3.0272531509399414, 2.911414861679077, 2.920255422592163, 3.0566365718841553, 3.1749534606933594, 3.0513916015625, 3.115476608276367, 3.0840466022491455, 3.0515360832214355, 2.987941026687622, 3.0038414001464844, 3.038910150527954, 3.0202829837799072, 3.016558885574341, 3.2455899715423584, 3.1052491664886475, 3.0572588443756104, 3.0515105724334717, 3.0377981662750244, 3.0452330112457275, 3.0991578102111816, 3.0559816360473633, 3.061357021331787, 3.059131383895874, 2.985342502593994, 3.0904595851898193, 3.08606219291687, 3.093388557434082, 3.007359743118286, 3.1333413124084473, 3.106509208679199, 3.0583105087280273, 3.1041414737701416, 3.1115424633026123, 3.001920223236084, 3.0732688903808594, 2.960318088531494, 2.9173600673675537, 3.0873351097106934, 3.0670406818389893, 3.0872550010681152, 3.0068886280059814, 3.0486221313476562, 3.019700050354004, 3.0560474395751953, 3.115384340286255, 3.035614013671875, 3.140674114227295, 3.0731594562530518, 3.0713987350463867, 3.139779806137085, 3.1249561309814453, 3.139899730682373, 3.066905975341797, 3.07547926902771, 3.0088305473327637, 3.132736921310425, 3.084552049636841, 3.0752618312835693, 3.0956380367279053, 3.0409109592437744, 3.1076512336730957, 3.073855400085449, 2.927483081817627, 3.093118190765381, 3.1720619201660156, 3.082768440246582, 3.3045554161071777, 3.031001329421997, 2.981029987335205, 3.020237684249878, 3.0408072471618652, 3.0860419273376465, 3.203688859939575, 3.0640110969543457, 3.0930042266845703, 3.1142187118530273, 2.9603514671325684, 3.0052123069763184, 2.873762607574463, 2.878046989440918, 3.041826009750366, 3.2118191719055176, 3.055739402770996, 3.123413562774658, 3.076700210571289, 3.0473227500915527, 2.9730708599090576, 2.9944913387298584, 3.018778085708618, 3.007549285888672, 3.0063607692718506, 3.346477746963501, 3.1338038444519043, 3.059032917022705, 3.0408663749694824, 3.0222554206848145, 3.044818162918091, 3.0835349559783936, 3.0572776794433594, 3.07576322555542, 3.080035448074341, 2.9513967037200928, 3.070218324661255, 3.0603528022766113, 3.0891590118408203, 2.998645544052124, 3.147351026535034, 3.109659194946289, 3.0609912872314453, 3.1192171573638916, 3.1319079399108887, 2.978424072265625, 3.082627534866333, 2.9208109378814697, 2.867196559906006, 3.08172869682312, 3.055131435394287, 3.084787368774414, 2.9841830730438232, 3.046999454498291, 2.9930124282836914, 3.0566089153289795, 3.1343297958374023, 3.0260303020477295, 3.13624906539917, 3.081401824951172, 3.0839779376983643, 3.187434673309326, 3.1606287956237793, 3.152599573135376, 3.0711240768432617, 3.068906545639038, 3.0155811309814453, 3.175739049911499, 3.086963653564453, 3.070141553878784, 3.0942633152008057, 3.026381492614746, 3.1261751651763916, 3.0495266914367676, 2.8793816566467285, 3.0667808055877686, 3.1512882709503174, 3.0499534606933594, 3.326303720474243, 3.005032777786255, 2.9523653984069824, 2.9960477352142334, 3.016653537750244, 3.058884620666504, 3.1768507957458496, 3.0357415676116943, 3.0659289360046387, 3.0995562076568604, 2.9201819896698, 2.969935894012451, 2.8359713554382324, 2.845785140991211, 3.0155398845672607, 3.181818723678589, 3.0284786224365234, 3.097358465194702, 3.0441455841064453, 3.0314817428588867, 2.940817356109619, 2.9581944942474365, 2.9858312606811523, 2.973806619644165, 2.979299545288086, 3.330946207046509, 3.1058387756347656, 3.0345535278320312, 3.0159218311309814, 2.993760347366333, 3.0161967277526855, 3.0519063472747803, 3.029135227203369, 3.049832582473755, 3.0592989921569824, 2.9126152992248535, 3.0449182987213135, 3.0341525077819824, 3.069214105606079, 2.96368408203125, 3.113215446472168, 3.0816402435302734, 3.0311684608459473, 3.0939278602600098, 3.112666606903076, 2.9379377365112305, 3.051417827606201, 2.8816776275634766, 2.820495367050171, 3.050798177719116, 3.019838333129883, 3.057678699493408, 2.9465723037719727, 3.0256717205047607, 2.9558475017547607, 3.03145432472229, 3.1114063262939453, 2.9969310760498047, 3.1079041957855225, 3.0579919815063477, 3.0631470680236816, 3.1793885231018066, 3.1155495643615723, 3.110666036605835, 3.040452718734741, 3.03928279876709, 3.003910541534424, 3.1495163440704346, 3.0555853843688965, 3.0443732738494873, 3.0742266178131104, 3.007148027420044, 3.1062674522399902], \"type\": \"scatter\", \"uid\": \"0caab848-81f0-4c1b-ae8c-d97ef979dae8\"}], {\"title\": {\"text\": \"batch/loss\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"ec72cba7-a561-4ea7-8049-ee5e616f8f14\")) {window._Plotly.Plots.resize(document.getElementById(\"ec72cba7-a561-4ea7-8049-ee5e616f8f14\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"ec72cba7-a561-4ea7-8049-ee5e616f8f14\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"ec72cba7-a561-4ea7-8049-ee5e616f8f14\")) {\n",
       "    Plotly.newPlot(\"ec72cba7-a561-4ea7-8049-ee5e616f8f14\", [{\"name\": \"train/batch/loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], \"y\": [3.167451858520508, 3.162532091140747, 3.15464448928833, 3.1606154441833496, 3.146578311920166, 3.15742826461792, 3.1474761962890625, 3.1663851737976074, 3.144502878189087, 3.1639976501464844, 3.150343894958496, 3.139404535293579, 3.1671159267425537, 3.14121150970459, 3.124532699584961, 3.1540186405181885, 3.1349425315856934, 3.1556379795074463, 3.1527435779571533, 3.13120698928833, 3.1529042720794678, 3.1199073791503906, 3.1332168579101562, 3.1448779106140137, 3.141596794128418, 3.1380515098571777, 3.1274564266204834, 3.145880699157715, 3.101627826690674, 3.1542067527770996, 3.1539969444274902, 3.148508310317993, 3.1634819507598877, 3.133809804916382, 3.138721466064453, 3.150353193283081, 3.1173691749572754, 3.1176722049713135, 3.136475086212158, 3.1123249530792236, 3.1671605110168457, 3.1263725757598877, 3.122121810913086, 3.1426544189453125, 3.1178863048553467, 3.127592086791992, 3.1111955642700195, 3.1351895332336426, 3.1124300956726074, 3.1237497329711914, 3.125225782394409, 3.1260149478912354, 3.1143271923065186, 3.1206130981445312, 3.0933518409729004, 3.1029062271118164, 3.1574935913085938, 3.136378526687622, 3.1053285598754883, 3.106792449951172, 3.0892813205718994, 3.098935842514038, 3.1085591316223145, 3.1085927486419678, 3.1118991374969482, 3.11784291267395, 3.0874624252319336, 3.1045303344726562, 3.1117937564849854, 3.1020867824554443, 3.120518445968628, 3.1075103282928467, 3.139374256134033, 3.0963070392608643, 3.1089253425598145, 3.1292924880981445, 3.0958240032196045, 3.102844715118408, 3.095151662826538, 3.119011878967285, 3.1006410121917725, 3.095897674560547, 3.082656145095825, 3.0877692699432373, 3.089799404144287, 3.091919422149658, 3.101266384124756, 3.083491086959839, 3.083766460418701, 3.106652021408081, 3.1088266372680664, 3.0897436141967773, 3.0532262325286865, 3.088233709335327, 3.088864803314209, 3.090498685836792], \"type\": \"scatter\", \"uid\": \"6e9aae70-9108-48f8-9079-65ecea75af04\"}, {\"name\": \"val/batch/loss\", \"x\": [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329], \"y\": [3.076899766921997, 2.9726452827453613, 3.1033992767333984, 3.153714418411255, 3.0845162868499756, 3.1733736991882324, 3.0366413593292236, 2.991420269012451, 3.029853105545044, 3.0561575889587402, 3.089643716812134, 3.1641247272491455, 3.069410800933838, 3.096703290939331, 3.107998847961426, 3.005009174346924, 3.0272531509399414, 2.911414861679077, 2.920255422592163, 3.0566365718841553, 3.1749534606933594, 3.0513916015625, 3.115476608276367, 3.0840466022491455, 3.0515360832214355, 2.987941026687622, 3.0038414001464844, 3.038910150527954, 3.0202829837799072, 3.016558885574341, 3.2455899715423584, 3.1052491664886475, 3.0572588443756104, 3.0515105724334717, 3.0377981662750244, 3.0452330112457275, 3.0991578102111816, 3.0559816360473633, 3.061357021331787, 3.059131383895874, 2.985342502593994, 3.0904595851898193, 3.08606219291687, 3.093388557434082, 3.007359743118286, 3.1333413124084473, 3.106509208679199, 3.0583105087280273, 3.1041414737701416, 3.1115424633026123, 3.001920223236084, 3.0732688903808594, 2.960318088531494, 2.9173600673675537, 3.0873351097106934, 3.0670406818389893, 3.0872550010681152, 3.0068886280059814, 3.0486221313476562, 3.019700050354004, 3.0560474395751953, 3.115384340286255, 3.035614013671875, 3.140674114227295, 3.0731594562530518, 3.0713987350463867, 3.139779806137085, 3.1249561309814453, 3.139899730682373, 3.066905975341797, 3.07547926902771, 3.0088305473327637, 3.132736921310425, 3.084552049636841, 3.0752618312835693, 3.0956380367279053, 3.0409109592437744, 3.1076512336730957, 3.073855400085449, 2.927483081817627, 3.093118190765381, 3.1720619201660156, 3.082768440246582, 3.3045554161071777, 3.031001329421997, 2.981029987335205, 3.020237684249878, 3.0408072471618652, 3.0860419273376465, 3.203688859939575, 3.0640110969543457, 3.0930042266845703, 3.1142187118530273, 2.9603514671325684, 3.0052123069763184, 2.873762607574463, 2.878046989440918, 3.041826009750366, 3.2118191719055176, 3.055739402770996, 3.123413562774658, 3.076700210571289, 3.0473227500915527, 2.9730708599090576, 2.9944913387298584, 3.018778085708618, 3.007549285888672, 3.0063607692718506, 3.346477746963501, 3.1338038444519043, 3.059032917022705, 3.0408663749694824, 3.0222554206848145, 3.044818162918091, 3.0835349559783936, 3.0572776794433594, 3.07576322555542, 3.080035448074341, 2.9513967037200928, 3.070218324661255, 3.0603528022766113, 3.0891590118408203, 2.998645544052124, 3.147351026535034, 3.109659194946289, 3.0609912872314453, 3.1192171573638916, 3.1319079399108887, 2.978424072265625, 3.082627534866333, 2.9208109378814697, 2.867196559906006, 3.08172869682312, 3.055131435394287, 3.084787368774414, 2.9841830730438232, 3.046999454498291, 2.9930124282836914, 3.0566089153289795, 3.1343297958374023, 3.0260303020477295, 3.13624906539917, 3.081401824951172, 3.0839779376983643, 3.187434673309326, 3.1606287956237793, 3.152599573135376, 3.0711240768432617, 3.068906545639038, 3.0155811309814453, 3.175739049911499, 3.086963653564453, 3.070141553878784, 3.0942633152008057, 3.026381492614746, 3.1261751651763916, 3.0495266914367676, 2.8793816566467285, 3.0667808055877686, 3.1512882709503174, 3.0499534606933594, 3.326303720474243, 3.005032777786255, 2.9523653984069824, 2.9960477352142334, 3.016653537750244, 3.058884620666504, 3.1768507957458496, 3.0357415676116943, 3.0659289360046387, 3.0995562076568604, 2.9201819896698, 2.969935894012451, 2.8359713554382324, 2.845785140991211, 3.0155398845672607, 3.181818723678589, 3.0284786224365234, 3.097358465194702, 3.0441455841064453, 3.0314817428588867, 2.940817356109619, 2.9581944942474365, 2.9858312606811523, 2.973806619644165, 2.979299545288086, 3.330946207046509, 3.1058387756347656, 3.0345535278320312, 3.0159218311309814, 2.993760347366333, 3.0161967277526855, 3.0519063472747803, 3.029135227203369, 3.049832582473755, 3.0592989921569824, 2.9126152992248535, 3.0449182987213135, 3.0341525077819824, 3.069214105606079, 2.96368408203125, 3.113215446472168, 3.0816402435302734, 3.0311684608459473, 3.0939278602600098, 3.112666606903076, 2.9379377365112305, 3.051417827606201, 2.8816776275634766, 2.820495367050171, 3.050798177719116, 3.019838333129883, 3.057678699493408, 2.9465723037719727, 3.0256717205047607, 2.9558475017547607, 3.03145432472229, 3.1114063262939453, 2.9969310760498047, 3.1079041957855225, 3.0579919815063477, 3.0631470680236816, 3.1793885231018066, 3.1155495643615723, 3.110666036605835, 3.040452718734741, 3.03928279876709, 3.003910541534424, 3.1495163440704346, 3.0555853843688965, 3.0443732738494873, 3.0742266178131104, 3.007148027420044, 3.1062674522399902], \"type\": \"scatter\", \"uid\": \"0caab848-81f0-4c1b-ae8c-d97ef979dae8\"}], {\"title\": {\"text\": \"batch/loss\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"ec72cba7-a561-4ea7-8049-ee5e616f8f14\")) {window._Plotly.Plots.resize(document.getElementById(\"ec72cba7-a561-4ea7-8049-ee5e616f8f14\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "name": "train/batch/lr",
         "type": "scatter",
         "uid": "18e96ba1-375c-48ba-b189-e85bb33a9d48",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95
         ],
         "y": [
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06,
          9.999999747378752e-06
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "batch/lr"
        },
        "yaxis": {
         "hoverformat": ".6f"
        }
       }
      },
      "text/html": [
       "<div id=\"cb217bdd-499c-4e5b-85be-2369d3b5e55b\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"cb217bdd-499c-4e5b-85be-2369d3b5e55b\")) {\n",
       "    Plotly.newPlot(\"cb217bdd-499c-4e5b-85be-2369d3b5e55b\", [{\"name\": \"train/batch/lr\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95], \"y\": [9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06], \"type\": \"scatter\", \"uid\": \"18e96ba1-375c-48ba-b189-e85bb33a9d48\"}], {\"title\": {\"text\": \"batch/lr\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"cb217bdd-499c-4e5b-85be-2369d3b5e55b\")) {window._Plotly.Plots.resize(document.getElementById(\"cb217bdd-499c-4e5b-85be-2369d3b5e55b\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"cb217bdd-499c-4e5b-85be-2369d3b5e55b\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"cb217bdd-499c-4e5b-85be-2369d3b5e55b\")) {\n",
       "    Plotly.newPlot(\"cb217bdd-499c-4e5b-85be-2369d3b5e55b\", [{\"name\": \"train/batch/lr\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95], \"y\": [9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06, 9.999999747378752e-06], \"type\": \"scatter\", \"uid\": \"18e96ba1-375c-48ba-b189-e85bb33a9d48\"}], {\"title\": {\"text\": \"batch/lr\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"cb217bdd-499c-4e5b-85be-2369d3b5e55b\")) {window._Plotly.Plots.resize(document.getElementById(\"cb217bdd-499c-4e5b-85be-2369d3b5e55b\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keker.plot_kek(logdir=\"logdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from kekas Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe creation and train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'PetImages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-9657eaa37c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# some files there are corrupted, so add only good ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                 \u001b[0;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(pathobj, *args)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PetImages'"
     ]
    }
   ],
   "source": [
    "# Let's create a pandas DataFrame to help us with data handling\n",
    "root_dir = Path(\"PetImages/\")  # path to Cats and Dogs dataset root directory\n",
    "\n",
    "fpaths = []\n",
    "labels = []\n",
    "for d in root_dir.iterdir():\n",
    "    for f in d.iterdir():\n",
    "        img = cv2.imread(str(f))  # some files there are corrupted, so add only good ones\n",
    "        if img is not None:\n",
    "            labels.append(d.name)\n",
    "            fpaths.append(str(f))\n",
    "\n",
    "df = pd.DataFrame(data={\"fpath\": fpaths, \"label\": labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split dataset to train and val parts\n",
    "train_df, val_df = train_test_split(df, test_size=2000)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and val datasets using DataKek class - a pytorch Dataset that uses pandas DataFrame as data source\n",
    "\n",
    "# at first we need to create a reader function that will define how image will be opened\n",
    "def reader_fn(i, row):\n",
    "    # it always gets i and row as parameters\n",
    "    # where i is an index of dataframe and row is a dataframes row\n",
    "    image = cv2.imread(row[\"fpath\"])[:,:,::-1]  # BGR -> RGB\n",
    "    if row[\"label\"] == \"Dog\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return {\"image\": image, \"label\": label}\n",
    "\n",
    "\n",
    "# Then we should create transformations/augmentations\n",
    "# We will use awesome https://github.com/albu/albumentations library\n",
    "def augs(p=0.5):\n",
    "    return Compose([\n",
    "        CLAHE(),\n",
    "        RandomRotate90(),\n",
    "        Transpose(),\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n",
    "        Blur(blur_limit=3),\n",
    "        OpticalDistortion(),\n",
    "        GridDistortion(),\n",
    "        HueSaturationValue()\n",
    "    ], p=p)\n",
    "\n",
    "def get_transforms(dataset_key, size, p):\n",
    "    # we need to use a Transformer class to apply transformations to DataKeks elements\n",
    "    # dataset_key is an image key in dict returned by reader_fn\n",
    "    \n",
    "    PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n",
    "\n",
    "    AUGS = Transformer(dataset_key, lambda x: augs()(image=x)[\"image\"])\n",
    "\n",
    "    NRM_TFMS = transforms.Compose([\n",
    "        Transformer(dataset_key, to_torch()),\n",
    "        Transformer(dataset_key, normalize())\n",
    "    ])\n",
    "    \n",
    "    train_tfms = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n",
    "    val_tfms = transforms.Compose([PRE_TFMS, NRM_TFMS])  # because we don't want to augment val set yet\n",
    "    \n",
    "    return train_tfms, val_tfms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataKeks creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create DataKeks\n",
    "train_tfms, val_tfms = get_transforms(\"image\", 224, 0.5)\n",
    "\n",
    "train_dk = DataKek(df=train_df, reader_fn=reader_fn, transforms=train_tfms)\n",
    "val_dk = DataKek(df=val_df, reader_fn=reader_fn, transforms=val_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and DataLoaders\n",
    "batch_size = 32\n",
    "workers = 8\n",
    "\n",
    "train_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=workers, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple neural network using pretrainedmodels library\n",
    "# https://github.com/Cadene/pretrained-models.pytorch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes: int,\n",
    "            p: float = 0.5,\n",
    "            pooling_size: int = 2,\n",
    "            last_conv_size: int = 2048,\n",
    "            arch: str = \"se_resnext50_32x4d\",\n",
    "            pretrained: str = \"imagenet\") -> None:\n",
    "        \"\"\"A simple model to finetune.\n",
    "        \n",
    "        Args:\n",
    "            num_classes: the number of target classes, the size of the last layer's output\n",
    "            p: dropout probability\n",
    "            pooling_size: the size of the result feature map after adaptive pooling layer\n",
    "            last_conv_size: size of the flatten last backbone conv layer\n",
    "            arch: the name of the architecture form pretrainedmodels\n",
    "            pretrained: the mode for pretrained model from pretrainedmodels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        net = pm.__dict__[arch](pretrained=pretrained)\n",
    "        modules = list(net.children())[:-2]  # delete last layers: pooling and linear\n",
    "        \n",
    "        # add custom head\n",
    "        modules += [nn.Sequential(\n",
    "            # AdaptiveConcatPool2d is a concat of AdaptiveMaxPooling and AdaptiveAveragePooling \n",
    "            AdaptiveConcatPool2d(size=pooling_size),\n",
    "            Flatten(),\n",
    "            nn.BatchNorm1d(2 * pooling_size * pooling_size * last_conv_size),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(2 * pooling_size * pooling_size * last_conv_size, num_classes)\n",
    "        )]\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the three whales of your pipelane are: the data, the model and the loss (hi, Jeremy)\n",
    "\n",
    "# the data is represented in Kekas by DataOwner. It is a namedtuple with three fields:\n",
    "# 'train_dl', 'val_dl', 'test_dl'\n",
    "# For training process we will need at least two of them, and we can skip 'test_dl' for now\n",
    "# so we will initialize it with `None` value.\n",
    "dataowner = DataOwner(train_dl, val_dl, None)\n",
    "\n",
    "# model is just a pytorch nn.Module, that we created vefore\n",
    "model = Net(num_classes=2)\n",
    "\n",
    "# loss or criterion is also a pytorch nn.Module. For multiloss scenarios it can be a list of nn.Modules\n",
    "# for our simple example let's use the standart cross entopy criterion\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also we need to specify, what model will do with each batch of data on each iteration\n",
    "# We should define a `step_fn` function\n",
    "# The code below repeats a `keker.default_step_fn` code to provide you with a concept of step function\n",
    "\n",
    "def step_fn(model: torch.nn.Module,\n",
    "            batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Determine what your model will do with your data.\n",
    "\n",
    "    Args:\n",
    "        model: the pytorch module to pass input in\n",
    "        batch: the batch of data from the DataLoader\n",
    "\n",
    "    Returns:\n",
    "        The models forward pass results\n",
    "    \"\"\"\n",
    "    \n",
    "    # you could define here whatever logic you want\n",
    "    inp = batch[\"image\"]  # here we get an \"image\" from our dataset\n",
    "    return model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous preparations was mostly out of scope of Kekas library (except DataKeks creation)\n",
    "# Now let's dive into kekas a little bit\n",
    "\n",
    "# firstly, we create a Keker - the core Kekas class, that provides all the keks for your pipeline\n",
    "keker = Keker(model=model,\n",
    "              dataowner=dataowner,\n",
    "              criterion=criterion,\n",
    "              step_fn=step_fn,                    # previosly defined step function\n",
    "              target_key=\"label\",                 # remember, we defined it in the reader_fn for DataKek?\n",
    "              metrics={\"acc\": accuracy},          # optional, you can not specify any metrics at all\n",
    "              opt=torch.optim.Adam,               # optimizer class. if note specifiyng, \n",
    "                                                  # an SGD is using by default\n",
    "              opt_params={\"weight_decay\": 1e-5})  # optimizer kwargs in dict format (optional too)\n",
    "\n",
    "# Actually, there are a lot of params for kekers, but this out of scope of this example\n",
    "# you can read about them in Keker's docstring (but who really reads the docs, huh?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before the start of the finetuning procedure let's freeeze all the layers except the last one - the head\n",
    "# the `freeze` method is mostly inspired (or stolen) from fastai\n",
    "# but you should define a model's attribute to deal with\n",
    "# for example, our model is actually model.net, so we need to specify the 'net' attr\n",
    "# also this method does not freezes batchnorm layers by default. To change this set `freeze_bn=True`\n",
    "keker.freeze(model_attr=\"net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's find an 'optimal' learning rate with learning rate find procedure\n",
    "# for details please see the fastai course and this articles:\n",
    "# https://arxiv.org/abs/1803.09820\n",
    "# https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n",
    "\n",
    "# NOTE: this is an optional step and you can skip it and use your favorite learning rate\n",
    "\n",
    "# you MUST specify the logdir to see graphics\n",
    "# keker will write a tensorboard logs into this folder\n",
    "# to see them start a tensorboard with `--logdir /path/to/logdir`\n",
    "# OR you can use keker.plot_kek_lr method (see cell below)\n",
    "keker.kek_lr(final_lr=0.1, logdir=\"/path/to/logdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Learning Rate find results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in plot to see on which step the loss was still decreasing\n",
    "# and choose LR from this step\n",
    "keker.plot_kek_lr(logdir=\"/path/to/logdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Kek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now let's start training!\n",
    "# It's as simple as:\n",
    "keker.kek(lr=1e-5, epochs=3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kek with different optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeKekasUser: Wait, and what if I want to train with the different optimizer?\n",
    "#\n",
    "# Me:\n",
    "keker.kek(lr=1e-5, \n",
    "          epochs=1,\n",
    "          opt=torch.optim.RMSprop,            # optimizer class\n",
    "          opt_params={\"weight_decay\": 1e-5})  # optimizer kwargs in dict format (if you want)\n",
    "\n",
    "# by default, the optimizer specified on Keker initialization is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kek with scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeKekasUser: OK, and what if I want to use a pytorch scheduler?\n",
    "#\n",
    "# Me:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=2,\n",
    "          sched=torch.optim.lr_scheduler.StepLR,       # pytorch lr scheduler class\n",
    "          sched_params={\"step_size\":1, \"gamma\": 0.9})  # schedulres kwargas in dict format\n",
    "\n",
    "# by default, no scheduler is using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log your keks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeKekasUser: How about logging?\n",
    "#\n",
    "# Me:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=1,\n",
    "          logdir=\"/mnt/hdd3_4/belskikh/keks/forplot\")\n",
    "\n",
    "# It will create a `train` and `val` subfolders in logdir, and will write tensorboard logs into them\n",
    "# to see them start a tensorboard with `--logdir /path/to/logdir`\n",
    "# OR you can use keker.plot_kek method! (see cell below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot your keks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kekas uses plotly lib and tensorboard logs to plot inside NB\n",
    "keker.plot_kek(logdir=\"/path/to/logdir\",  # path to logdir with logs to plot\n",
    "               step=\"batch\",              # (optional) default is \"step\". another option is \"epoch\"\n",
    "                                          # It determines discreteness of ploting\n",
    "               metrics=[\"loss\",           # (optional) list of metrics names\n",
    "                        \"acc\",            # by default [\"loss\", \"lr\"] is using\n",
    "                        \"lr\"],            # the order of the names determines the order of the plot\n",
    "                                          # NOTE: names of metrics must match names in metrics dict\n",
    "                                          # which was specified on Keker init step\n",
    "               height=1200,               # (optional) height of the total plot \n",
    "               width=800)                 # (optional) width of the total plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeKekasUser: Also I want to save best checkpoints to later use them for SWA or ensembling!\n",
    "#                And I want to measure them by custom metric, control their number, specify their name prefix,\n",
    "#                and control what I need - minimize or maximize metric!\n",
    "# Me: Here it is:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=1,\n",
    "          cp_saver_params={\n",
    "              \"savedir\": \"/path/to/save/dir\",  # a directory for checkpoints\n",
    "              \"metric\": \"acc\",  # (optional) from `metrics` dict on Keker init. \n",
    "                                # default is validation loss\n",
    "              \"n_best\": 3,      # (optional) default is 3\n",
    "              \"prefix\": \"kek\",  # (optional) default prefix is `checkpoint`\n",
    "              \"mode\": \"max\"     # (optional) default is 'min'\n",
    "          })   \n",
    "\n",
    "# It will create a `savedir` directory, and will save best checkpoints there\n",
    "# with naming `{prefix}.{epoch_num}.h5`. The best checkpoint will be dublicated with `{prefix}.best.h5` name\n",
    "# look at the report down here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeKekasUser: Allright, and I don't want to train model, if validation loss doesn't improve for several epochs.\n",
    "# \n",
    "# Me: You mean, early stopping? Here:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=1, \n",
    "          early_stop_params={\n",
    "              \"patience\": 3,   # number of bad epochs to wait before stopping\n",
    "              \"metric\": \"acc\", # (optional) metric name from 'metric' dict. default is val loss\n",
    "              \"mode\": \"min\",   # (optional) what you want from you metric, max or min? default is 'min'\n",
    "              \"min_delta\": 0   # (optional) a minimum delta to count an epoch as 'bad'\n",
    "          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeAdvancedKekasUser: I WANT IT ALL!\n",
    "# \n",
    "# Me: Well, okay then...\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=5,\n",
    "          opt=torch.optim.RMSprop,\n",
    "          opt_params={\"weight_decay\": 1e-5},\n",
    "          sched=torch.optim.lr_scheduler.StepLR,\n",
    "          sched_params={\"step_size\":1, \"gamma\": 0.9},\n",
    "          logdir=\"/path/to/logdir\",\n",
    "          cp_saver_params={\n",
    "              \"savedir\": \"/path/to/save/dir\",  \n",
    "              \"metric\": \"acc\",  \n",
    "              \"n_best\": 3,      \n",
    "              \"prefix\": \"kek\",  \n",
    "              \"mode\": \"max\"},     \n",
    "          early_stop_params={\n",
    "              \"patience\": 3,   \n",
    "              \"metric\": \"acc\", \n",
    "              \"mode\": \"min\",   \n",
    "              \"min_delta\": 0\n",
    "          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Cycle Kek!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SomeFastaiFan: Did you stole something else from fastai?\n",
    "#\n",
    "# Me: Yes! One Cycle Policy!\n",
    "keker.kek_one_cycle(max_lr=1e-5,                  # the maximum learning rate\n",
    "                    cycle_len=5,                  # number of epochs, actually, but not exactly\n",
    "                    momentum_range=(0.95, 0.85),  # range of momentum changes\n",
    "                    div_factor=25,                # max_lr / min_lr\n",
    "                    increase_fraction=0.3)        # the part of cycle when learning rate increases\n",
    "\n",
    "# If you don't understand these parameters, read this - https://sgugger.github.io/the-1cycle-policy.html\n",
    "# NOTE: you cannot use schedulers and early stopping with one cycle!\n",
    "# another options are the same as for `kek` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Keker features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing / unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've already talk about freezing. But what if I want to unfreeze?\n",
    "# It has the same interface:\n",
    "keker.unfreeze(model_attr=\"net\")\n",
    "\n",
    "# If you want to freeze till some layer:\n",
    "layer_num = -2\n",
    "keker.freeze_to(layer_num, model_attr=\"net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "keker.save(\"/path/to/file\")\n",
    "\n",
    "# loading\n",
    "keker.load(\"/path/to/file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device and DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keker is using all avialable GPUs by default\n",
    "# To limit it, use 'CUDA_VISIBLE_DEVICES' environment variable (available in os.environ dict)\n",
    "\n",
    "# if you want to specify cuda device for your model, specify `device` parameter on Keker initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 4 (yes, four) ways to get a predictions with keker\n",
    "\n",
    "# 1st\n",
    "keker.predict(savepath=\"/path/to/save/dir\")\n",
    "# it will makes predicts on your 'test_dl' dataloader (remember, we initialized it with 'None'), if it specified,\n",
    "# and saves models output in numpy.ndarray format to 'savepath'\n",
    "\n",
    "# 2nd\n",
    "loader = val_dl\n",
    "keker.predict_loader(loader=loader, savepath=\"/path/to/save/dir\")\n",
    "# it will do the same as `predict()` but on any custom loader you want\n",
    "\n",
    "# 3rd\n",
    "tensor = torch.zeros(4, 224, 224, 3)\n",
    "preds = keker.predict_tensor(tensor=tensor, to_numpy=False)\n",
    "# it will return a predictions of the model in numpy format if `'to_numpy==True', else - torch.Tensor\n",
    "\n",
    "# 4th\n",
    "array = np.zeros((4, 224, 224, 3))\n",
    "preds = keker.predict_array(array=array, to_numpy=False)\n",
    "# it will do the same as `predict_tensor()` but with np.ndarra as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am sure that it is not very convinient way for test time augmentations,\n",
    "# but here is how you can do it with Kekas\n",
    "\n",
    "# first, specify several augmentations for TTA\n",
    "flip_ = Flip(always_apply=True)\n",
    "vertical_flip_ = VerticalFlip(always_apply=True)\n",
    "transpose_ = Transpose(always_apply=True)\n",
    "\n",
    "# second, create the whole augmentations with theese ones inside\n",
    "def insert_aug(aug, dataset_key=\"image\", size=224):    \n",
    "    PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n",
    "    \n",
    "    AUGS = Transformer(dataset_key, lambda x: aug(image=x)[\"image\"])\n",
    "    \n",
    "    NRM_TFMS = transforms.Compose([\n",
    "        Transformer(dataset_key, to_torch()),\n",
    "        Transformer(dataset_key, normalize())\n",
    "    ])\n",
    "    \n",
    "    tfm = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n",
    "    return tfm\n",
    "\n",
    "\n",
    "flip = insert_aug(flip_)\n",
    "vertical_flip = insert_aug(vertical_flip_)\n",
    "transpose = insert_aug(transpose_)\n",
    "\n",
    "tta_tfms = {\"flip\": flip, \"v_flip\": vertical_flip, \"transpose\": transpose}\n",
    "\n",
    "# third, run TTA\n",
    "keker.TTA(loader=val_dl,                # loader to predict on \n",
    "          tfms=tta_tfms,                # list or dict of always applying transforms\n",
    "          savedir=\"/path/to/save/dir\",  # savedir\n",
    "          prefix=\"preds\")               # (optional) name prefix. default is 'preds'\n",
    "\n",
    "# it will saves predicts for each augmentation to savedir with name\n",
    "#  - {prefix}_{name_from_dict}.npy if tfms is a dict\n",
    "#  - {prefix}_{index}.npy          if tfms is a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks is the way in which Kekas customizes its pipeline\n",
    "# each callback implements six methods, which names tell when it applies\n",
    "# on_train_begin()\n",
    "#     on_epoch_begin()\n",
    "#         on_batch_begin()\n",
    "#             >>>... step here ...<<<\n",
    "#         on_batch_end()\n",
    "#     on_epoch_end()\n",
    "# on_train_end()\n",
    "\n",
    "# Callbacks are widely using under the hood of Kekas\n",
    "# For example - loss, opimizer, progressbar, lr scheduling, checkpoint saving, early stopping etc\n",
    "# are realized as callbacks\n",
    "\n",
    "# Callback has access to `state` attr of a keker. Here is a docs from Keker about state:\n",
    "\n",
    "        # The state is an object that stores many variables and represents\n",
    "        # the state of your train-val-repdict pipeline. _state passed to every\n",
    "        # callback call.\n",
    "        # You can use it as a container for your custom variables, but\n",
    "        # DO NOT USE the following ones:\n",
    "        #\n",
    "        # loss, batch, model, dataowner, criterion, opt, parallel, checkpoint,\n",
    "        # stop_iter, stop_epoch, stop_train, out, sched, mode, loader, pbar,\n",
    "        # metrics, epoch_metrics\n",
    "\n",
    "# You can write your own callback, or use something useful from kekas.callbacks\n",
    "\n",
    "# Callbacks should be passes as a list at the Keker initiation\n",
    "# For example, let's use a DebuggerCallback, that just insert a pdb.set_trace() call in pipeline\n",
    "# For more info, please see a DebuggerCallback docs and source code\n",
    "debugger = DebuggerCallback(when=[\"on_epoch_begin\"], modes[\"train\"])\n",
    "\n",
    "keker = Keker(model=model, dataowner=dataowner, criterion=criterion, callbacks=[debugger])\n",
    "\n",
    "# also there is a method to add a callbacks to existing Keker\n",
    "\n",
    "keker.add_callbacks([debugger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss and opimizer callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As was said, loss and optimezer behavior is realiesed as Callbacks.\n",
    "# If you use some tricky loss or optimizer logic, you can create your own Callback\n",
    "# and specify it during Keker initialization\n",
    "\n",
    "# here are the callbacks, that are using by default\n",
    "class LossCallback(Callback):\n",
    "    def __init__(self, target_key: str, preds_key: str) -> None:\n",
    "        # target_key and preds_key are the parameters of Keker\n",
    "        self.target_key = target_key\n",
    "        self.preds_key = preds_key\n",
    "\n",
    "    def on_batch_end(self, i: int, state: DotDict) -> None:\n",
    "        target = state.batch[self.target_key]\n",
    "        preds = state.out[self.preds_key]\n",
    "\n",
    "        state.loss = state.criterion(preds, target)\n",
    "\n",
    "class OptimizerCallback(Callback):\n",
    "    def on_batch_end(self, i: int, state: DotDict) -> None:\n",
    "        if state.mode == \"train\":\n",
    "            state.opt.zero_grad()\n",
    "            state.loss.backward()\n",
    "            state.opt.step()\n",
    "            \n",
    "# and here is how you should specify them during Keker initialization\n",
    "keker = Keker(model=model, \n",
    "              dataowner=dataowner,\n",
    "              criterion=criterion,\n",
    "              loss_cb=LossCallback,\n",
    "              opt_cb=OptimizerCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you now got an idea how to use Kekas.\n",
    "\n",
    "I will be happy to get feedback about my library and this tutorial.\n",
    "\n",
    "You can find me in [OpenDataScience](http://ods.ai) community by @belskikh nikname or create an issue on GitHub.\n",
    "\n",
    "Have a good keks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
